\documentclass[]{article}

\usepackage{chronology}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tocloft}
\usepackage{cancel}
\usepackage{thmtools}
\usepackage[toc,nonumberlist,acronym]{glossaries}
\usepackage{glossaries-extra}
\usepackage{relsize}
\usepackage{physics}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{cor}{Corollary}
\graphicspath{{figs/}}
\widowpenalty10000
\clubpenalty10000
\setcounter{tocdepth}{2}

\makeglossaries
%opening
\title{Computational Neuroscience Notes}
\author{Simon Crase}

\begin{document}
	
\newacronym{gls:EEG}{EEG\glsadd{gls:eeg}}{\Gls{gls:eeg}}

\newacronym{gls:EPSP}{EPSP}{Excitatory Post Synaptic Potential}

\newacronym{gls:fMRI}{fMRI\glsadd{gls:fmri}}{\Gls{gls:fmri}}

\newacronym{gls:LGN}{LGN\glsadd{gls:lgn}}{\Gls{gls:lgn}}

\newacronym{gls:LTD}{LTD\glsadd{gls:ltd}}{\Gls{gls:ltd}}

\newacronym{gls:LTP}{LTP\glsadd{gls:ltp}}{\Gls{gls:ltp}}

\newacronym{gls:MAP}{MAP\glsadd{gls:mpm}}{\Gls{gls:mpm}}

\newacronym{gls:ML}{ML\glsadd{gls:mlm}}{\Gls{gls:mlm}}

\newacronym{gls:PCA}{PCA}{Principal Component Analysis}

\newacronym{gls:STDP}{STDP}{Spike-Timing Dependent Plasticity}

\newglossaryentry{gls:action:potential}{
	name={action potential},
	description={A rapid sequence of changes in the voltage across a membrane\cite{grider2022physiology}}
}

\newglossaryentry{gls:correlation:matrix}{
	name={correlation matrix},
	description={The correlation matrix of $n$ random variables $\{X_i\}$ is the $n\times n$ matrix $Q$\cite{enwiki:1123244609}
	\begin{align*}
		q_{ij} =& \frac{cov(X_i,X_j)}{\sigma_{X_i}\sigma_{X_j}} \text{, or, using \eqref{eq:covariance:matrix}}\\
		=& \frac{C_{i,j}}{\sigma_{X_i}\sigma_{X_j}}
	\end{align*} }}

\newglossaryentry{gls:covariance:matrix}{
	name={covariance matrix},
	description={A square matrix giving the covariance between each pair of elements of a given random vector. Any covariance matrix is symmetric and positive semi-definite and its main diagonal contains variances (i.e., the covariance of each element with itself)\cite{enwiki:1123552471}.
		\begin{align*}
			C_{ij}=\mathbb{E}\big((X_i-\mathbb{E}(x_i))(X_j-\mathbb{E}(x_j))\big) \numberthis \label{eq:covariance:matrix}
	\end{align*} }}


\newglossaryentry{gls:depolarization}{
	name={depolarization},
	description={positive change in voltage}}

\newglossaryentry{gls:eeg}{
	name={Electroencephalography},
	description={a method to record an electrogram of the spontaneous electrical activity of the brain. The biosignals detected by EEG have been shown to represent the postsynaptic potentials of pyramidal neurons in the neocortex and allocortex. It is typically non-invasive, with the EEG electrodes placed along the scalp (commonly called "scalp EEG") using the International 10-20 system, or variations of it. Electrocorticography, involving surgical placement of electrodes, is sometimes called "intracranial EEG". Clinical interpretation of EEG recordings is most often performed by visual inspection of the tracing or quantitative EEG analysis.\cite{enwiki:1128901597}}}

\newglossaryentry{gls:electrotonic}{
	name={electrotonic},
	description={of, relating to, or being the spread of electrical activity through living tissue or cells in the absence of repeated \glspl{gls:action:potential}\cite{webster2022electrotonic}}}


\newglossaryentry{gls:fmri}{
	name={Functional magnetic resonance imaging},
	description={Functional magnetic resonance imaging or functional MRI (fMRI) measures brain activity by detecting changes associated with blood flow. This technique relies on the fact that cerebral blood flow and neuronal activation are coupled. When an area of the brain is in use, blood flow to that region also increases\cite{enwiki:1124875371}}}

\newglossaryentry{gls:hyperpolarization}{
	name={hyperpolarization},
	description={negative change in voltage}}

\newglossaryentry{gls:lgn}{
	name = {lateral geniculate nucleus},
	description = {A multilayered structure that receives input from both eyes to build a representation of the contralateral visual hemifield\cite{Felleman2001}}}

\newglossaryentry{gls:ltd}{
	name={Long Term Depression},
	description={Experimentally observed decrease in synaptic strength that lasts for hours or days}}

\newglossaryentry{gls:ltp}{
	name={Long Term Potentiation},
	description={Experimentally observed increase in synaptic strength that lasts for hours or days}
}

\newglossaryentry{gls:mlm}{
	name={maximum likelihood},
	description={Choose estimator $s^*$ to maximize $p[r\vert s]$
		}}
	
\newglossaryentry{gls:mpm}{
	name={maximum a posteriori},
	description={Choose estimator $s^*$ to maximize $p[s \vert r]$
}}

\newglossaryentry{gls:rf}{
	name={receptive field},
	description={Specific properties of a sensory stimulus that generate a strong response from the cell\cite{coursera2017cns}}}


\maketitle

\begin{abstract}
My notes from Computational Neuroscience course\cite{coursera2017cns}, which provides an introduction to basic computational methods for understanding what nervous systems do and for determining how they function.
\end{abstract}

\tableofcontents

\section{Introduction and Basic Neurobiology}\label{sec:week1}

\subsection{Course Introduction}

Understanding the Brain using Computational Models
\begin{itemize}
	\item Descriptive Models of the Brain
	\begin{itemize}
		\item How do neurons respond to external stimuli and how do we
		describe this quantitatively with a neural encoding model?
		\item How can we extract information from neurons (decoding)?
	\end{itemize}
    \item Mechanistic Models of Brain Cells and Networks
	\begin{itemize}
		\item     How can we simulate the behavior of a single neuron on a
	    computer?
	    \item How do we simulate a network of neurons?
	\end{itemize}
	\item  Interpretive (or Normative) Models of the Brain
	\begin{itemize}
		\item 	Why do brain circuits operate the way they do?
		\item What are the computational principles underlying their
		operation?
	\end{itemize}
\end{itemize}

\subsection{Computational Neuroscience - Descriptive Models}

\subsubsection{\Glspl{gls:rf}}

\begin{figure}[H]
	\caption[Responses of a Neuron in an Intact Cat Brain]{Responses of a Neuron in an Intact Cat Brain\cite{hubel1965receptive}}
	\includegraphics[width=\textwidth]{receptive-field-cat}
\end{figure}

\begin{figure}[H]
	\caption{Responses showing \gls{gls:rf}}
	\includegraphics[width=\textwidth]{receptive-field-cat-bars}
\end{figure}

\subsubsection{\Glspl{gls:rf}: a Descriptive Model}

\begin{figure}[H]
	\caption{\Glspl{gls:rf} in the Retina}
	\includegraphics[width=0.8\textwidth]{receptive-fields-in-the-retina}
\end{figure}

\begin{figure}[H]
	\caption{Centre Surround \Glspl{gls:rf}}
	\includegraphics[width=0.8\textwidth]{center-surround}
\end{figure}

\begin{figure}[H]
	\caption{Cortical\Glspl{gls:rf}}
	\includegraphics[width=\textwidth]{CorticalReceptive Fields}
\end{figure}

\begin{figure}[H]
	\caption{Oriented \gls{gls:rf} of a neuron in primary visual cortex (V1)}
	\includegraphics[width=\textwidth]{orientation-preference}
\end{figure}

\begin{figure}[H]
	\caption[How are these oriented \glspl{gls:rf} obtained?]{How are these \glspl{gls:rf} obtained from center-surround \glspl{gls:rf}?}\label{fig:rf-shape}
	\includegraphics[width=\textwidth]{orientation-preference2}
\end{figure}

\subsection{Computational Neuroscience Mechanistic and Interpretive Models}


\begin{figure}[H]
	\caption[\Glspl{gls:rf}: a Mechanistic Model]{\Glspl{gls:rf}: a Mechanistic Model}
	\includegraphics[width=0.9\textwidth]{mech-rf}
\end{figure}

\begin{figure}[H]
	\caption[Model suggested by Hubel \& Wiesel in the 	1960s]{Model suggested by 	Hubel \& Wiesel in the 	1960s: V1 RFs are created from converging 	\gls{gls:LGN} inputs. Center-surround \gls{gls:LGN} \glspl{gls:rf} are displaced along preferred orientation of V1 cell This simple model is still controversial!}
	\includegraphics[width=0.9\textwidth]{mech-rf-v1}
\end{figure}

\subsubsection{\Glspl{gls:rf}: an Interpretive Model}
Why are \glspl{gls:rf} in V1 shaped as in Figure \ref{fig:rf-shape}?

\begin{figure}[H]
	\caption[Efficient Coding Hypothesis]{Efficient Coding Hypothesis: suppose the
		goal is to represent images as faithfully and efficiently as possible using neurons with \glspl{gls:rf} $RF_1, RF_2$, etc}
	\begin{subfigure}[t]{0.2\textwidth}
		\caption{$RF_1$}
		\includegraphics[width=0.7\textwidth]{rf1}
	\end{subfigure}
	\begin{subfigure}[t]{0.2\textwidth}
		\caption{$RF_2$}
		\includegraphics[width=\textwidth]{rf2}
	\end{subfigure}
	\begin{subfigure}[t]{0.2\textwidth}
		\caption{$RF_3$}
		\includegraphics[width=\textwidth]{rf3}
	\end{subfigure}
	\begin{subfigure}[t]{0.2\textwidth}
		\caption{$RF_4$}
		\includegraphics[width=0.5\textwidth]{rf4}
	\end{subfigure}
\end{figure}

Given image $I$, we can reconstruct it using neural responses $\{r_i\}$.
\begin{align*}
	\hat{I} =& \sum RF_i r_i
\end{align*}

What are the $RF_i$ that minimize the total squared pixelwise errors between $I$ and $\hat{I}$ and are as independent as possible?

\begin{figure}[H]
	\begin{center}
		\caption[Interpretive Model of \Glspl{gls:rf}]{Interpretive Model of \Glspl{gls:rf}: start out with random $RF_i$ and run your efficient coding algorithm on natural image patches}
		\begin{subfigure}[t]{0.9\textwidth}
			\begin{center}
				\caption{Natural Images}
				\includegraphics[width=0.7\textwidth]{natural-image}
			\end{center}
		\end{subfigure}
		\begin{subfigure}[t]{\textwidth}
			\caption{Receptive Fields from Natural Images}
			\includegraphics[width=\textwidth]{rf-natural}
		\end{subfigure}
		\begin{subfigure}[t]{\textwidth}
			\caption{Receptive Fields from V1, after Figure \ref{fig:rf-shape}}
			\includegraphics[width=\textwidth]{orientation-preference2}
		\end{subfigure}
	\end{center}
\end{figure}

 The brain may be trying to find faithful and efficient representations of an animal’s natural environment\cite{olshausen1997sparse,bell1997independent,rao1999predictive}

\subsection{The Electrical Personality of Neurons}

Originally there were two competing hypothesis:
\begin{itemize}
	\item the reticular hypothesis supposed that the brain was a continuous network;
	\item the \emph{neuron doctrine}:
	\begin{itemize}
		\item the neuron is the fundamental structural \& functional unit of the brain;
		\item neurons are discrete cells and not continuous with other cells;
		\item information flows from the dendrites to the axon via the cell body.
	\end{itemize}
\end{itemize}

\begin{figure}[H]
	\caption{The Electrical Personality of Neurons}
	\begin{subfigure}[t]{0.45\textwidth}
		\caption{The idealized Neuron. The \glspl{gls:EPSP} are summed: if the sum reaches a threshold, we get an \gls{gls:action:potential}, also known as a spike.}
		\includegraphics[width=\textwidth]{idealized-neuron}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\begin{center}
			\caption{A neuron is a ``leaky bag of charged liquid''}\label{fig:what:is:a:neuron}
			\includegraphics[width=\textwidth]{what-is-a-neuron}
		\end{center}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\begin{center}
			\caption{The Electrical Personality of a Neuron}\label{fig:what:is:a:neuron2}
			\includegraphics[width=\textwidth]{what-is-a-neuron2}
		\end{center}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\caption{Ionic Channels: the Gate Keepers.  Ionic channels  are selective and	allow only specific ions to pass through}\label{fig:ionic-channel}
		\includegraphics[width=\textwidth]{ionic-channel}
	\end{subfigure}
\end{figure}

\begin{itemize}
	\item  A neuron is a ``leaky bag of charged liquid''--Figure \ref{fig:what:is:a:neuron}
	\item Contents of the neuron enclosed within a cell membrane
	\item Cell membrane is a lipid bilayer
	\begin{itemize}
		\item Bilayer is impermeable to charged ion species such as $Na^+$ , $Cl^-$, and $K^+$
		\item Ionic channels embedded in membrane allow ions to flow in or out
	\end{itemize}
	\item Each neuron maintains a potential difference across its membrane--Figure \ref{fig:what:is:a:neuron2};
	\item Inside is about –70 mV relative to outside;
	\begin{itemize}
		\item sodium and chloride higher outside
		\item potassium and organic anions [$A^-$] higher inside
	\end{itemize}
	\item Ionic pump maintains -70 mV difference by expelling [$Na^+$] 
	and allowing [$K^+$] ions in. A lot of the energy that we consume is used to operate ionic pump.
	\item Ionic channels in membranes are proteins that are selective and allow only specific ions to pass through--Figure \ref{fig:ionic-channel}
	\item E.g. Pass $Na^+$ but not $K^+$ or $Cl^-$.
	\item Ionic channels are gated
	\begin{itemize}
		\item Voltage-gated: Probability of opening depends on membrane voltage
		\item Chemically-gated: Binding to a chemical causes channel to open--Figure \ref{fig:signalling}
		\item Mechanically-gated: Sensitive to 	pressure or stretch
	\end{itemize}
\end{itemize}

\begin{figure}[H]
	\caption[Gated Channels allow Neuronal Signaling]{Gated Channels allow Neuronal Signaling. The junctions between neurons are known as ``synapses''}\label{fig:signalling}
	\includegraphics[width=0.9\textwidth]{signalling}
\end{figure}

\begin{itemize}
	\item Inputs from other neurons $\rightarrow$ 	chemically-gated channels (at
	``synapse'') open  $\rightarrow$ Changes in local membrane potential
	\item This in turn causes opening/closing 	of voltage-gated channels in 	dendrites, body, and axon, resulting in \gls{gls:depolarization} (\glsdesc{gls:depolarization}) or \gls{gls:hyperpolarization}	(\glsdesc{gls:hyperpolarization})
	\item Strong enough \gls{gls:depolarization} causes a spike or \gls{gls:action:potential}
\end{itemize}

\begin{figure}[H]
	\caption[The Output of a Neuron:\Gls{gls:action:potential} (Spike)]{The Output of a Neuron: \Gls{gls:action:potential} (Spike). Voltage-gated channels cause \glspl{gls:action:potential} (spikes) Strong \gls{gls:depolarization} opens $Na^+$ channels, causing rapid $Na^+$ influx and more channels to open, until they inactivate; $K^+$ outflux restores membrane potential. The shape of the \gls{gls:action:potential} is stereotypical, so no information is propagated by the shape.}
	\includegraphics[width=0.7\textwidth]{action-potential}
\end{figure}

\begin{figure}[H]
	\caption[Propagation of a Spike along an Axon]{Propagation of a Spike along an Axon. At each point, the $Na^+$ rushes in first, followed by the $K^+$ rushing out\cite{krantz2000PsychScholar}}
	\includegraphics[width=0.7\textwidth]{propagation1}
\end{figure}



\begin{figure}[H]
	\caption[Active Wiring: Myelination of Axons]{Active Wiring: Myelination of Axons. Myelin due to oligodendrocytes (glial cells) wrap axons and
		enable fast long-range spike communication
		Action potential “hops” from one non-myelinated region
		(node of Ranvier) to the next (saltatory conduction)
		“Active wire” allows lossless signal propagation}
	\includegraphics[width=0.9\textwidth]{myelination}
\end{figure}



\subsection{Making Connections - Synapses}
\begin{figure}[H]
	\caption[Enter the Synapse]{What happens to the spike (action potential) when	it reaches the 	end of an axon?	Enter the Synapse}\label{fig:enter:the:synapse}
	\includegraphics[width=0.9\textwidth]{enter-synapse}
\end{figure}

\begin{figure}[H]
	\caption[What is a Synapse?]{What is a Synapse?  a connection or junction between two neurons: Electrical synapses use gap junctions;	Chemical synapses use neurotransmitters.}
	\includegraphics[width=0.9\textwidth]{synapse1}
\end{figure}

Why two mechanisms? Why would we evolve slow chemical synapses in addition to fast electrical ones? One possible answer is that we can adjust strength of coupling by changing the number of vesicles; it has been suggested that chemical synapses are the basis for learning and memory.
\begin{figure}[H]
	\caption[Distribution of synapses on a real neuron]{Distribution of synapses on a real neuron}
	\includegraphics[width=0.9\textwidth]{synapse2}
\end{figure}

\begin{figure}[H]
	\caption[Synapses can be Excitatory or Inhibitory]{Synapses can be Excitatory or Inhibitory: increase or decrease postsynaptic membrane potential}
	\includegraphics[width=0.9\textwidth]{synapse3}
\end{figure}

\begin{figure}[H]
	\caption[An Excitatory Synapse]{An Excitatory Synapse. Input spike  $\rightarrow$
		Neurotransmitter 	release (e.g., Glutamate)  $\rightarrow$ Binds to ion channel receptors $\rightarrow$ Ion channels open $\rightarrow$ $Na^+$ influx  $\rightarrow$ 	\Gls{gls:depolarization} due to \gls{gls:EPSP}}
	\includegraphics[width=0.9\textwidth]{synapse4}
\end{figure}

\begin{itemize}
	\item The Synapse Doctrine: Synapses are the basis for memory and learning
	\item Hebbian Plasticity--Figure \ref{fig:hebbian-plasticity}
	\item \acrfull{gls:LTP}:  \glsdesc{gls:ltp}
	\item \acrfull{gls:LTD}:  \glsdesc{gls:ltd}
\end{itemize}

\glsdesc{gls:ltd}

\begin{figure}[H]
	\begin{center}
		\caption[Hebbian Plasticity]{Hebbian Plasticity: if neuron A repeatedly takes part in firing neuron B, then the synapse from A to B is strengthened; neurons that fire together wire together! Section \ref{sec:week7:1}}\label{fig:hebbian-plasticity}
		\includegraphics[width=0.9\textwidth]{hebbian-plasticity}
	\end{center}
\end{figure}

\begin{figure}[H]
	\caption[Synaptic Plasticity depends on Spike Timing]{Synaptic Plasticity depends on Spike Timing. LTP/LTD depends on relative timing of input \& output spikes.}
	\includegraphics[width=0.9\textwidth]{ltp-ltd}
\end{figure}

\begin{figure}[H]
	\caption[\acrfull{gls:STDP}]{\acrfull{gls:STDP})\cite{bi2001synaptic}}
	\includegraphics[width=0.9\textwidth]{stdp}
\end{figure}



\subsection{Time to Network - Brain Areas and their Function}

\begin{figure}[H]
	\caption{Brain Regions}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Hind Brain. \emph{Medulla Oblongata} controls breathing, muscle tone and blood pressure.
			\emph{Pons} connected to the cerebellum \& involved in sleep and arousal.
			\emph{Cerebellum} Coordination and timing of voluntary movements, sense of equilibrium, language, attention}
		\includegraphics[width=\textwidth]{hindbrain}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{\emph{Midbrain} Eye movements, visual and auditory reflexes.
			\emph{Reticular Formation} 	Modulates muscle reflexes, breathing \& pain perception. Also regulates sleep, 			wakefulness \& 	arousal.}
		\includegraphics[width=\textwidth]{midbrain}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{\emph{Thalamus} Relay station for all 	sensory info (except 	smell) to the cortex, regulates sleep/wakefulness.
		\emph{Hypothalamus} Regulates basic needs: 	Fighting, Fleeing, 	Feeding, and Mating.}
		\includegraphics[width=\textwidth]{thalamus}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{ Consists of: Cerebral cortex, basal ganglia, 	hippocampus, and amygdala
		Involved in perception 	and motor control, 	cognitive functions,	emotion, memory, and learning}
		\includegraphics[width=\textwidth]{Cerebrum}
	\end{subfigure}
\end{figure}

\section{What do Neurons Encode?}\label{sec:week2}

\subsection{What is the Neural Code}
During this course, we will talk about:

\begin{itemize}
	\item techniques for recording from the brain--Figure \ref{fig:recording};
	\item  tools for discovering how the brain represents information;
	\item models that express our understanding of this representation;
	\item  some methods for inferring what the brain is doing based on its activity--Section \ref{sec:week3};
	\item  using information theory to quantify neural representations--Section \ref{sec:week4};
	\item the biophysical basis of how the brain processes inputs and performs complex computations--Section \ref{sec:week5}.
\end{itemize}

\begin{figure}[H]
	\caption[Recording from the Brain]{Recording from the Brain. (\subref{fig:rb1}) \& (\subref{fig:rb2}) \gls{gls:fMRI}: resolution $1\;mm^3$; response is averaged over many neurons, and is slow. (\subref{fig:rb3}) \gls{gls:EEG}: response is averaged over many neurons; EEG is faster than fMRI, but noisy. (\subref{fig:rb4}) \& (\subref{fig:rb5}) Electrode Arrays: good if we have access to tissue directly\label{fig:recording} (\subref{fig:rb6}) \& (\subref{fig:rb7}) Calcium Imaging. Cells have markers that change the colour of fluorescence as calcium levels change in response to neural activity.} 
	\begin{subfigure}[t]{0.45\textwidth}
		\caption{}\label{fig:rb1}
		\includegraphics[width=0.9\textwidth]{fMRI}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\caption{}\label{fig:rb2}
		\includegraphics[width=0.9\textwidth]{fMRI2}
	\end{subfigure}
	\begin{subfigure}[t]{0.3\textwidth}
		\caption{}\label{fig:rb3}
		\includegraphics[width=0.9\textwidth]{EEG}
	\end{subfigure}
	\begin{subfigure}[t]{0.3\textwidth}
		\caption{}\label{fig:rb4}
		\includegraphics[width=0.9\textwidth]{electrode-arrays}
	\end{subfigure}
	\begin{subfigure}[t]{0.3\textwidth}
		\caption{}\label{fig:rb5}
		\includegraphics[width=0.9\textwidth]{electrode-arrays2}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\caption{}\label{fig:rb6}
		\includegraphics[width=0.9\textwidth]{calcium-imaging1}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\caption{}\label{fig:rb7}
		\includegraphics[width=0.9\textwidth]{calcium-imaging2}
	\end{subfigure}
\end{figure}



\begin{figure}[H]
	\begin{center}
		\caption[Looking inside a single cell]{Looking inside a single cell. The experimenter clamps a patch electron onto the cell membrane}
		\includegraphics[width=0.9\textwidth]{looking-inside}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption{What is the neural code?}
		\begin{subfigure}[t]{0.45\textwidth}
			\caption{Human eye, with output to optic nerve}
			\includegraphics[width=\textwidth]{human-eye}
		\end{subfigure}
		\begin{subfigure}[t]{0.45\textwidth}
			\caption{Experiment. Play movie while section of retina is connected to electrode array--Figures \ref{fig:rb4} \& \ref{fig:rb5}}\label{fig:movie}
			\includegraphics[width=\textwidth]{human-eye-experiment}
		\end{subfigure}
		\begin{subfigure}[t]{0.45\textwidth}
			\caption{Repeat experiment (\subref{fig:movie}): red dots represent firings (action potentials). Notice that many times the firings are in almost the same place.}
			\includegraphics[width=\textwidth]{movie-firings}
		\end{subfigure}
		\begin{subfigure}[t]{0.45\textwidth}
			\caption{Look at 20 retinal ganglion cells. Each cell is reasonably consistent in time. Each cell is responsible for encoding some set of features, and different neurons encode different features. R \& P have some features in common.}\label{fig:movie-firings-20}
			\includegraphics[width=\textwidth]{movie-firings-20}
		\end{subfigure}
	\end{center}
\end{figure}

\begin{itemize}
	\item Encoding: how does a stimulus cause a pattern of responses?
	\begin{itemize}
		\item  Building quasi mechanistic models
	\end{itemize}
	\item Decoding: what do these responses tell us about the stimulus?
	\begin{itemize}
		\item how can we reconstruct what the brain is doing?
	\end{itemize}
\end{itemize}

\begin{align*}
	P(response\vert stimulus)&\text{, encoding} \numberthis \label{eq:p_r_s}\\
	P(stimulus\vert response)&\text{, decoding}  \numberthis \label{eq:p_s_r}
\end{align*}

\begin{itemize}
	\item What is the stimulus--$s$?
	\item What is the response--$r$?
	\item what is the relation between them?
\end{itemize}
\begin{figure}[H]
	\begin{center}
		\caption[Gaussian tuning curve of a cortical (V1) neuron]{Gaussian tuning curve of a cortical (V1) neuron. These neurons respond to oriented bars.}
		\includegraphics[width=0.9\textwidth]{tuning-curves}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption{Cosine tuning curve of a motor cortical neuron}
		\includegraphics[width=0.9\textwidth]{tuning-curves1}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption{Feature Map in Primary Visual Cortex}
		\includegraphics[width=0.9\textwidth]{feature-map}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption{Higher order features in temporal lobe}
		\includegraphics[width=0.9\textwidth]{higher-order-features}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption[Building up more complex features]{Building up more complex features. Notice feed-forward and feed-back.}
		\includegraphics[width=0.9\textwidth]{build}
	\end{center}
\end{figure}

\subsection{Neural Encoding - Simple Models}

We  consider the case where a response is a single spike. We want to determine $P(R\vert S) \rightarrow r(t)$, where $r(t)$ is the firing rate, given a stimulus $s$.

\begin{figure}[H]
	\begin{center}
		\caption{Simplest possible model: $r(t) = \phi s(t-\tau)$)}
		\includegraphics[width=0.9\textwidth]{simplest-model}
	\end{center}
\end{figure}

\subsubsection{What about a linear filter?}

\begin{align*}
	r(t) =& \sum_{i=1}^{n} s_{t-i} f_i \text{, or spatial filtering} \numberthis \label{eq:linear:filter}\\
	r(x,y) =& \sum_{x^\prime=-n,y^\prime=-n}^{n,n}s_{x-x^\prime,y-y^\prime} f_{x^\prime,y^\prime} \text{, as in Figure \ref{fig:rf1}}
\end{align*}

\begin{figure}[H]
	\begin{center}
		\caption[Receptive Field]{Receptive Field: (\subref{fig:rf1}) is a cartoon view, (\subref{fig:rf2}) shows the filter as a difference between two Gaussians; (\subref{fig:rf4}) shows the effect of applying the filter to (\subref{fig:rf3}.)}
		\begin{subfigure}[t]{0.45\textwidth}
			\caption{Cartoon}\label{fig:rf1}
			\includegraphics[width=0.9\textwidth]{receptive-field1}
		\end{subfigure}
		\begin{subfigure}[t]{0.45\textwidth}
			\caption{Detailed view}\label{fig:rf2}
			\includegraphics[width=0.9\textwidth]{receptive-field2}
		\end{subfigure}
		\begin{subfigure}[t]{0.45\textwidth}
			\caption{Taj Mahal}\label{fig:rf3}
			\includegraphics[width=0.9\textwidth]{receptive-field3}
		\end{subfigure}
		\begin{subfigure}[t]{0.45\textwidth}
			\caption{Filtered Taj Mahal, showing edges only}\label{fig:rf4}
			\includegraphics[width=0.9\textwidth]{receptive-field4}
		\end{subfigure}
	\end{center}
\end{figure}

\begin{figure}[H]
	\caption[Spatiotemporal filtering]{Spatiotemporal filtering. $r_{x,y}(t)=\int dx^\prime dy^\prime d\tau f(x^\prime,y^\prime,\tau) s((x-x^\prime,y-y^\prime,t-\tau)$}
	\includegraphics[width=0.9\textwidth]{spatio--temporal-filtering}
\end{figure}

\subsubsection{Linear filters cannot be the full picture}

Linear filters have a few limitations:
\begin{itemize}
	\item they can give rise to a negative firing rate!
	\item they can increase indefinitely!
\end{itemize} 

We need to impose a non-linearity, so \eqref{eq:linear:filter} becomes:
\begin{align*}
	r(t) =& g\big(\sum_{i=1}^{n} s_{t-i} f_i\big) \numberthis \label{eq:non-linearity}
\end{align*}

\begin{figure}[H]
	\caption{Filter showing effect of saturating non-linearity}
	\includegraphics[width=0.9\textwidth]{saturated}
\end{figure}

\subsection{Neural Encoding - Feature Selection}
How to find the components of \eqref{eq:non-linearity}? Our problem is dimensionality! We want to sample the responses of the system to many stimuli, so we can characterize what it is about the input that drives responses. We will start with a very high dimensional description and pick out a small set of relevant dimensions.

\begin{figure}[H]
	\caption[Dimensionality Reduction]{Dimensionality Reduction: $P(response|stimulus)\rightarrow P(response\vert s_1,s_2,...s_n)$}
	\includegraphics[width=0.9\textwidth]{dimenionality-reduction}
\end{figure}

Gaussian white noise is a useful method.

\begin{figure}[H]
	\caption[Determining multiple features from white noise]{Determining multiple features from white noise. This allows us to create a Gaussian prior. This will be a multivariate Gaussian, no matter which axes we use.}\label{fig:determining-multiple-features}
	\includegraphics[width=0.9\textwidth]{white-noise}
\end{figure}

\begin{figure}[H]
	\caption[Spike Conditioned Distribution]{Spike Conditioned Distribution. We project onto a vector representing mean of spike-conditioned points}\label{fig:spike:conditioned}
	\includegraphics[width=0.9\textwidth]{spike-conditioned-distribution}
\end{figure}

\begin{figure}[H]
	\caption[Reverse Correlation: the spike triggered average]{Reverse Correlation: the spike triggered average. Every time there is a spike, grab the portion of signal that preceded it, forming an ensemble, which we average. This represents what is common to stimuli that triggered response.}
	\includegraphics[width=0.9\textwidth]{reverse-correlation}
\end{figure}

\begin{figure}[H]
	\caption[Spike Triggered average for vector]{Spike Triggered average for vector, maybe in image that has been turned into a single vector}
	\includegraphics[width=0.9\textwidth]{sta-image}
\end{figure}

In each of these cases we have a vector that represents a feature, and we project the stimulus onto this unit vector. How do we compute the input-output curve, the function $g$ from \eqref{eq:non-linearity}? 

\begin{align*}
	P(spike\vert stimulus) \rightarrow& P(spike\vert s_1) \text{. This can be found using Bayes' rule.}\\
	 P(spike\vert s_1) =& \frac{\overbrace{P(s_1\vert spike)}^\text{Factor from Figure \ref{fig:factors:bayes}} \cdot \overbrace{P(spike)}^\text{Prior from Figure \ref{fig:determining-multiple-features}}}{\underbrace{P(s_1)}_\text{Factor from Figure \ref{fig:factors:bayes}}}
\end{align*}


\begin{figure}[H]
	\caption[Factors used in Bayes Rule]{Factors used in Bayes Rule}\label{fig:factors:bayes}
	\includegraphics[width=0.9\textwidth]{compondents-of bayes}
\end{figure}


\begin{figure}[H]
	\caption[Examples of IO curves]{Examples of IO curves. The left hand curve shows no impact from the stimulus; either there is none, or we have chosen the wrong vector.}\label{fig:sample-io-curves}
	\includegraphics[width=0.9\textwidth]{sample-io-curves}
\end{figure}

\subsubsection{High-dimensional Feature Selection}

We select some subset of features that are relevant to a decision: $r(t)=g(f_1*s, f_2*x,...)$. How can we extract more information from Figure \ref{fig:spike:conditioned}?

\begin{figure}[H]
	\caption[High-dimensional Feature Selection]{High-dimensional Feature Selection. We could extract higher order moments, or the covariance. Can use \gls{gls:PCA} to reduce dimensionality.}
	\includegraphics[width=0.9\textwidth]{spike-conditioned-distribution-covariance}
\end{figure}

\begin{figure}[H]
	\caption[Most faces can be reconstructed from a set of 7-8 eigenfaces]{Although it takes a lot of pixels to represent a face, there is a lot of structure, and most faces can be reconstructed from a set of 7-8 principle components, known as ``eigenfaces''.}
	\includegraphics[width=0.9\textwidth]{eigenfaces}
\end{figure}

\begin{figure}[H]
	\caption[Use of \gls{gls:PCA} to sort out spikes]{Use of \gls{gls:PCA} to sort out spikes from two or more neurons recorded from a single electrode}
	\includegraphics[width=0.9\textwidth]{pca-spike-sorting}
\end{figure}

\begin{figure}[H]
	\caption[Finding Interesting Features in the Retina using \gls{gls:PCA}]{Finding Interesting Features in the Retina using \gls{gls:PCA}. Each blue dot represents 100 steps of a white-noise flicker. Spike triggered average is zero: we have two features, one which recognizes light turning on, the other off.}
	\includegraphics[width=0.9\textwidth]{finding-interestimg-features}
\end{figure}

\subsection{Neural Encoding - Variability}

There are a couple of things in Figure \ref{fig:movie-firings-20} that we still need to address: we modeled only a time varying firing rate, so thee are hidden assumptions about the rates at which signals and responses vary. There appears to be some fins structure that we have missed.

When have you found a good feature or features?
\begin{itemize}
	\item When the input/output curve over your variable is interesting
	\item How to quantify interesting?
\end{itemize}

\subsubsection{Quantifying Interesting}
In Figure \ref{fig:sample-io-curves}, the left hand curve is boring, the right interesting. Can we exploit the tuning curve and find a $f$ that maximizes the difference between the prior and posterior?

\begin{align*}
	P(spike\vert s_f) =&\frac{ P(s_f\vert spike) P_({spike})}{P(s_f)}
\end{align*}
 We introduce the Kullback-Leibler divergence
\begin{align*}
		D_{KL}(P(s),Q(s))\triangleq&\mathbb{E}_{P(s)} \Big(\log_2 \frac{P(s)}{Q(s)}\Big) \text{, and maximize}\\
		D_{KL}(P(s_f|spike),P(s_f))
\end{align*}

\begin{figure}[H]
	\caption[Maximally Informative Dimensions]{Maximally Informative Dimensions: choose filter to maximize $K_{DL}$ between spike-conditioned and prior distributions. This turns out to be equivalent to maximizing mutual information-\eqref{eq:mutual:information}. Notice that the stimulus is not necessarily Gaussian.}
	\includegraphics[width=0.9\textwidth]{maximally-informative-dimensions}
\end{figure}
This technique:
\begin{itemize}
	\item does not depend on white noise inputs;
	\item can be used for deriving models from natural stimuli;
	\item but is not guaranteed to give rise to a unique maximum.
\end{itemize}

Summary--finding relevant features:
\begin{enumerate}
	\item single filter determined by the conditional average;
	\item  a family of filters derived using \gls{gls:PCA};
	\item information theoretic methods use the whole distribution.
\end{enumerate}

\subsubsection{Modelling the noise}

We can model spikes that may or may not occur with a series of time bins, each of which may or may not contain a spike. In the limit where there are many bons we can use the Poisson distribution.

\begin{align*}
	P_T(k) =& (rT)^k\frac{\exp(-rT)}{k!}\\
	<k> =& rT\\
	var(k) =& rT\\
	F =& 1 \text{, Fano factor}\\
	P(T)=&r \exp(-rT)
\end{align*}

\begin{figure}[H]
	\caption[Poisson or not?]{Poisson or not? Monkey is watching a movie. Rate is variable. Split time into bins and plot mean number of spikes and variance in each bin. Firing rate changes, but Fano remains close to 1.}
	\includegraphics[width=0.9\textwidth]{poisson-or-not}
\end{figure}

\begin{figure}[H]
	\caption[Interspike Interval Distributions]{Interspike Interval Distributions. A typical cortical neuron is connected to 10,000 others: average input may be zero, but there will be some jitter. Right hand image zooms in on short intervals, which are no longer Poisson for a good reason: a neuron's firing rate is limited by physics.}
	\includegraphics[width=0.9\textwidth]{interspike-intervals}
\end{figure}

\begin{figure}[H]
	\caption[Generalized Linear Model]{Generalized Linear Model\cite{pillow2008spatio}. $P(spike\;at\;t) \propto \exp(f_1*s + h_1*r)$. The exponential nonlinearity allows all parameters to be determined using an optimization scheme that is globally convergent.}
	\includegraphics[width=0.9\textwidth]{generalized-linear-model}
\end{figure}

\begin{figure}[H]
	\caption[Coupled Spiking Model]{Coupled Spiking Model\cite{pillow2008spatio}. $P(spike\;at\;t) \propto \exp(f_1*s + h_1*r_1+h_2*r_2)$.}
	\includegraphics[width=0.9\textwidth]{coupled-spiking-model}
\end{figure}

\begin{figure}[H]
	\caption[Time Rescaling Theorem]{The Time Rescaling Theorem\cite{brown2002time} allows us to determine whether we have captured everything we can from the inputs to out model. Scale output time intervals by firing rate that model predicted. If predicted rate accounts for all the influences on the firing, then scaled intervals should be distributed like a pure Poisson process with an effective rate of 1.} 
	\includegraphics[width=0.9\textwidth]{time-rescaling-thorem}
\end{figure}

\section{Extracting Information from Neurons}\label{sec:week3}

\subsection{Neural Decoding and Signal Detection Theory}
See \cite{britten1992analysis}
\begin{figure}[H]
		\caption[Value of threshold that maximizes probability of correct call]{This value of threshold maximizes probability of calling correctly.
			$p[+]p[r\ge z\vert +] + p[-] (1-p[r\ge z\vert -])$}
		\includegraphics[width=\textwidth]{signal-detection1}
\end{figure}
\subsection{Population Coding and Bayesian Estimating}

\begin{align*}
	\underbrace{p[s\vert r]}_\text{A posteriori distribution} =& \frac{\overbrace{p[r\vert s]}^\text{Likelihood function} \cdot \overbrace{p[s]}^\text{Prior distribution}}{\underbrace{p[r]}_\text{Marginal distribution}} \text{, where}\\
	p[r] =& \int ds \; p[r\vert s] p[s]
\end{align*}

\begin{itemize}
	\item \gls{gls:ML} \gls{gls:mlm}
	\item \gls{gls:MAP} \gls{gls:mpm}
\end{itemize}
 An example. Assume:
 
\begin{enumerate}
	\item a population of neurons that encode some stimulus $s$
	\item response is Gaussian (e.g. V1)
	\item each fires independently
	\item Poisson Firing\label{item:poisson}
\end{enumerate}

\begin{figure}[H]
	\caption{Gaussian Tuning Curves}
	\includegraphics[width=\textwidth]{decode-stimulus}
\end{figure}

\begin{align*}
	f_a(s) =& r_{max} \exp \Big(-\frac{1}{2}\big[\frac{s-s_a}{\sigma_a}\big]^2\Big)\text{, assume good coverage} \numberthis \label{eq:gauss}\\
	\sum_{1}^{N} f_a(s) =& const \numberthis \label{eq:good:coverage}
\end{align*}

So firing rate doesn't depend on stimulus

From assumption \ref{item:poisson}, spikes are produced randomly and independently in each time bin with probability
\begin{align*}
	P_T[k] =& \frac{(rT)^k \exp (-rT)}{k!}\\
	P_T[r_a\vert s] =& \frac{(f_a(s)T)^{r_aT} \exp (-f_a(s)T)}{r_aT!}\\
	P[\vec{r}\vert s] =& \prod_{a=1}^{N}P_r[a\vert s]\\
	=& \prod_{a=1}^{N} \frac{(f_a(s)T)^{r_aT} \exp (-f_a(s)T)}{r_aT!}
\end{align*}

We want the \gls{gls:ML} for $s$.
\begin{align*}
	\ln P[\vec{r}\vert s] =& \sum_{a=1}^{N} \big[r_aT \ln (f_a(s)T) -f_a(s)T - \ln (r_aT!) \big] \text{, so we need}\\
	\nabla_a \ln P[\vec{r}\vert s] =& 0	
\end{align*}
 where  $\nabla_a$ denotes the operator $\frac{\partial}{\partial_{s_a}}$. Now
\begin{align*}
	\nabla_a \ln P[\vec{r}\vert s] = & \partial_a \sum_{a=1}^{N}  r_aT \ln (f_a(s)T) -\underbrace{\partial_a \sum_{a=1}^{N} f_a(s)T}_{=0 \text { from ]\eqref{eq:good:coverage}}} - \underbrace{\partial_a \sum_{a=1}^{N} \ln (r_aT!)}_{=0} 
\end{align*}
Hence the \gls{gls:mlm} is given by:
\begin{align*}
	T \partial_a \sum_{a=1}^{N} r_a  \ln (f_a(s)T) =& 0\\
	 \sum_{a=1}^{N} r_a  \partial_a \ln (f_a(s)T) =& 0\\
	 \sum_{a=1}^{N} r_a  \frac{\partial_a  (f_a(s)\cancel{T})}{(f_a(s)\cancel{T})} =& 0\\
	 \sum_{a=1}^{N} r_a  \frac{\partial_a  f_a(s^*)}{f_a(s^*)} =& 0 \numberthis \label{eq:ml:example}
\end{align*}

\begin{align*}
	\partial_a  f_a(s) =& \partial_a r_{max} \exp \Big(-\frac{1}{2}\big[\frac{s-s_a}{\sigma_a}\big]^2\Big)\\
	=&r_{max}\big[-\frac{1}{\cancel{2}} \frac{\cancel{2}(s-s_a)}{\sigma_a^2}\big] \exp \Big(-\frac{1}{2}\big[\frac{s-s_a}{\sigma_a}\big]^2\Big)\\
	=& \frac{\cancel{2}(s-s_a)}{\sigma_a^2} f_a(s) \text{, so \eqref{eq:ml:example} becomes}
\end{align*}
\begin{align*}
	\sum_{a=1}^{N} r_a  \frac{(s^*-s_a)}{\sigma_a^2} =& 0\\
	s^*  =& \frac{\sum_{a=1}^{N}   \frac{s_a r_a}{\sigma_a^2}}{\sum_{a=1}^{N} \frac{ r_a }{\sigma_a^2}}	\\
	=& \frac{\sum_{a=1}^{N}   p_a s_a r_a}{\sum_{a=1}^{N} p_a r_a} \text{, where the precision $p_a=\sigma_a^{-2}$}\\
	=& \frac{\sum_{a=1}^{N}   s_a r_a}{\sum_{a=1}^{N} r_a} \text{, if all the $s_a$ are equal}
\end{align*}

For the \gls{gls:MAP} estimate, Bayes rule gives:
\begin{align*}
	\ln p[s|r] =& \ln p[r|s] + \ln p[s] - \ln p[r]\\
	=& T \sum_{a=1}^{N}r_a \ln f_a(s) + \ln p[s] + C \text{, where $C$ does not depend on $s_a$}
\end{align*}

So the \gls{gls:MAP} estimator satisfies:
\begin{align*}
	\sum_{a=1}^{N} r_a \frac{f^\prime(a^*)}{f(a^*)} + \frac{p^\prime(s)}{p(s)}=&0\\
	s^*  =& \frac{\sum_{a=1}^{N}   \frac{s_a r_a}{\sigma_a^2}+\frac{s_{prior}}{\sigma_{prior}^2}}{\sum_{a=1}^{N} \frac{ r_a }{\sigma_a^2}+\frac{1}{\sigma_{prior}^2}}
\end{align*}
\subsection{Reading Minds - Stimulus Reconstruction}
We want an estimator, $S_{Bayes}$ that gives the "best" estimate of $s$ given $r$.

\section{Entropy \& Spike Trains}\label{sec:week4}

\subsection{Information \& Entropy}

Entropy measures surprise.
\begin{align*}
	H =& - \sum_{i} p_i \log_2 p_i
\end{align*}

\begin{figure}[H]
	\begin{center}
		\caption{How about the stimulus?}
		\includegraphics[width=0.8\textwidth]{how}\label{fig:how:about:the:stimulus}
	\end{center}
\end{figure}

In Figure \ref{fig:how:about:the:stimulus}, suppose the probability of error is a constant.
\begin{align*}
	P[r_-\vert +] =& q\\
	P[r_+\vert +] =& 1-q\\
	P[r_+\vert -] =& q\\
	P[r_-\vert -] =& 1-q
\end{align*}
\begin{align*}
	H[R] =& -P[r_+] \log P[r_+] - P[r_-] \log P[r_-] \text{, total entropy}\\
	H[R\vert +] =& -q \log q - (1-q) \log (1-q) \text{, noise entropy}
\end{align*}


Mutual Information: total entropy - average noise entropy.

\begin{align*}
	I(R,S) =& -\sum_r p[r] \log p[r] - \sum_s p(s)\big[- \sum_r p[r \vert s] \log p[r \vert s]\big]\\
	=& H[R] - \sum_s p(s) H[R\vert s] \numberthis \label{eq:mutual:information}\\
\end{align*}
\begin{align*}
	I[S,R] \triangleq D_{KL}&\big[P(R,S),P(R)P(S)\big]\\
	=&H[R]-\sum_s P(s) H[R\vert s]
\end{align*}

\subsection{Calculating Information in Spike Trains}

What information is carried by patterns of spikes?

Mutual  information  is  the  difference  between  
the  total  response  entropy  and   the  mean  noise  entropy.
\begin{align*}
	I(S;R) =& H[R] - \sum_s P[s] H[R\vert s]
\end{align*}
\begin{figure}[H]
	\begin{center}
			\caption[Calculating Information in Spike Patterns]{Calculating Information in Spike Patterns. Divide into words of length $T$, e.g. $w_1=[1,0,1,0,1,1,0]$. Information is difference between total variability driven by stimuli and that dues to noise, averaged over stimuli.\cite{strong1998entropy}}
		\includegraphics[width=\textwidth]{calculatingInformationInSpikePatterns}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption[Entropy for chunked words]{Entropy for chunked words. The most common pattern is all zeroes, then a single bit, etc. nformation  :   
			difference  between  the  total  
			variability   driven  by  stimuli  
			and  that  due  to   noise,  averaged  
			over  stimuli. \cite{strong1998entropy,reinagel2000temporal}}
		\includegraphics[width=\textwidth]{info-spike-trains}
	\end{center}
\end{figure}
\subsection{Coding Principles}
\begin{itemize}
	\item What are the challenges posed by natural stimuli?
	\item What do information theoretic concepts suggest that neural systems should do?
	\item What principle seems to be at work shaping the neuarl code?
\end{itemize}

Photo - have to change F-stop repeatedly in order to capture details of scene inside and outside; the eye does this effortlessly. Natural stimuli:
\begin{enumerate}
	\item Have a wide dynamic range: variations over many orders of magnitude;
	\item dynamic scaling.
\end{enumerate}

\begin{figure}[H]
	\begin{center}
		\caption[Power Law]{Power Law: there are similar structures at very different length scales.}
		\begin{subfigure}[b]{0.45\textwidth}
			\caption{Structures}
			\includegraphics[width=0.9\textwidth]{power-law-graph}
		\end{subfigure}
		\begin{subfigure}[b]{0.45\textwidth}
			\caption{Power Spectrum: there is no characteristic scale for image.}
			\includegraphics[width=0.9\textwidth]{power-law-shapes}
		\end{subfigure}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption[Efficient Coding]{Efficient Coding: In order to have maximum entropy output, a good encoder should match its outputs to the distribution of its inputs. Try to use all symbols equally often. (cumulative probability)}
		\includegraphics[width=0.8\textwidth]{efficient-coding}
	\end{center}
\end{figure}

Contrast varies widely in time. Should neural system optimize locally or over evolutionary time?

\begin{figure}[H]
	\begin{center}
		\caption{What have we left out}
		\includegraphics[width=0.8\textwidth]{4-big-picture}
	\end{center}
\end{figure}

Make sure we have covered
\begin{enumerate}
	\item Efficient coding
	\item Sparse Coding
\end{enumerate}

\begin{align*}
	I(\vec{x}) =& \sum_{i}a_i \phi_i(\vec{x}) + \epsilon(\vec{x})\\
	E =& \mathlarger \sum_{\vec{x}}\big[I(\vec{x})-\sum_{i}a_i \phi_i(\vec{x})\big]^2 + \lambda \sum_{i}C(a_i)
\end{align*}


\section{Computing in Carbon}\label{sec:week5}
How does the brain instantiate the kinds of computations we have seen in previous sections?
\begin{itemize}
	\item Neuroelectronics
	\begin{itemize}
		\item membranes
		\item ion channels
		\item wiring
	\end{itemize}
	\item Simplified Neuron Models (the basic dynamics of neuronal excitability)
	\item Neuronal Geometry (Dendrites and dendritic computing)
\end{itemize}

\subsection{Modeling Neurons}

\begin{figure}[H]
	\caption[Equivalent Circuit Model]{Equivalent Circuit Model: our goal is to develop a circuit diagram that models a neuron, using Kirchhoff's laws}
	\includegraphics[width=0.9\textwidth]{equivalent-circuit-model}
\end{figure}

In Figure \ref{fig:review:circuit} we review circuit theory.

\begin{itemize}
	\item The potential drop along a wires is zero;
	\item The current flowing into one element must issue the current flowing out;
	\item At a junction, the total current is zero (Kirchhoff's law);
	\item The potential changes by a fixed amount across a battery symbol;
	\item The voltage across a resistor obeys Ohm's law, $V=IR$ or $I=Vg$.
\end{itemize}

\begin{figure}[H]
	\caption[Review of Simple Circuit]{Review of Simple Circuit. If we regard the total current as flowing in through the top and out through the bottom, Kirchoff's Law requires that the sum of the currents in the two branches adds up to the total}\label{fig:review:circuit}
	\includegraphics[width=0.9\textwidth]{ReviewSimpleCircuit}
\end{figure}

\begin{figure}[H]
	\caption[Membrane Patch]{Membrane Patch after \cite{dayan2005theoretical}. The lipid bilayer is a good insulator, and the ion channels allows ions to pass selectively.}
	\includegraphics[width=0.9\textwidth]{membrane-patch}
\end{figure}

\begin{figure}[H]
	\caption[The passive membrane]{The passive membrane}
	\includegraphics[width=0.9\textwidth]{passive-membrane}
\end{figure}

\begin{align*}
	I_R + I_C + I_{ext} =& 0 \text{, Kirchhoff's Law}\\
	V =& I_R R \text{, Ohm's Law}\\
	I_C =& C \frac{dV}{dt} \text{, lipid bilayer acts as capacitor, plus allows a current to flow.}\\
	F\frac{dV}{dt} =& -\frac{V}{R} + I_{ext} \numberthis \label{eq:1st:neuron:eq}
\end{align*}


\begin{figure}[H]
	\caption[The Cell has a battery]{Although \eqref{eq:1st:neuron:eq} may model a piece of membrane, it isn't really what is happening in the cell: the membrane encloses a solution whose concentrations of ions are different from those outside.}
	\begin{subfigure}[b]{\textwidth}
		\caption{The Cell has a battery}\label{fig:cell:battery}
		\includegraphics[width=0.9\textwidth]{cell-has-battery}
	\end{subfigure}
	\begin{subfigure}[b]{\textwidth}
		\caption{The equilibrium potential. There are two opposing forces at work: ions move down their concentration gradient, until opposed by electrostatic forces.}\label{fig:equilibrium-potential}
		\includegraphics[width=0.9\textwidth]{equilibrium-potential}
	\end{subfigure}
\end{figure}


The Equilibrium of the osmotic and electrostatic forces in Figure \ref{fig:equilibrium-potential} is determined by the Nernst potential\cite{enwiki:1121504046}:
\begin{align*}
	E =& \frac{k_B T}{zq}\ln \frac{[inside]}{[outside]} \text{, where}\\
	[inside] =& \text{concentration of ions inside}\\
	[outside] =& \text{concentration of ions outside}\\
	k_B=& \text{Boltzmann's constant}\\
	T=& \text{ Temperature}\\
	q=& \text{ the ionic charge}\\
	z=&	\text{ the number of charges in the ion}
\end{align*}

The voltage across the resistor is lessened by $V_{rest}$ in Figure \ref{fig:cell:battery}, so \eqref{eq:1st:neuron:eq} becomes
\begin{align*}
		C\frac{dV}{dt} =& -\frac{(-V-V_{rest})}{R} + I_{ext} \text{, or}\\
	\tau \frac{dV}{dt} =& -V + V_\infty \text{, where the time constant $\tau$ is given by}\\
	\tau =& R C \text{, and} \\
	V_\infty =& \text{ represents the steady state}
\end{align*}

\begin{figure}[H]
	\caption[Example Solution for square wave $I_{ext}(t)$]{Example Solution for square wave $I_{ext}(t)$}
	\includegraphics[width=0.9\textwidth]{neuron-solution}
\end{figure}

Figure \ref{eg:ion-channels} depicts ion channels. There are different types: \begin{itemize}
	\item voltage dependent; 
	\item transmitter dependent (synaptic); 
	\item Ca dependent; 
	\item mechanosensitive;
	\item heat sensitive.
\end{itemize}

\begin{figure}[H]
	\caption[Currents through Ion Channels]{Currents through Ion channels. }\label{eg:ion-channels}\label{fig:ion-channels}
	\includegraphics[width=0.9\textwidth]{ion-channels}
\end{figure}

We will focus on voltage sensitivity, and will consider the current through one channel, $I=Vg$, where $g=\frac{1}{R}$ is the conductance of the channel. Different ion channels have different conductances; a given conductance tends to move the membrane potential towards the equilibrium potential for that ion.
\begin{table}[H]
	\begin{center}
		\caption[Examples of equilibrium potentials]{Examples of equilibrium potentials. Notice that sodium tends to depolarize, and potassium to polarize.}
		\begin{tabular}{|l|r|} \hline
			$E_{Na}$&50mV \\ \hline
			$E_{Ca}$&150mV \\ \hline
			$E_{K}$&-80mV \\ \hline
			$E_{Cl}$&-60mV \\ \hline
		\end{tabular}
	\end{center}
\end{table}

\begin{figure}[H]
	\caption[Current through ion channel]{Current through ion channel}
	\includegraphics[width=0.9\textwidth]{ion-channels3}
\end{figure}


\begin{figure}[H]
	\caption[What makes a neuron compute?]{What makes a neuron compute? So far everything we have seen is linear: the figure shows that most curves are just expanded or contracted versions of each other. But for some input currents, something else is happening...excitability}
	\includegraphics[width=0.9\textwidth]{what-makes-neuron-compute}
\end{figure}


\subsection{Spikes}

\begin{figure}[H]
	\caption[Excitability arises from non-linearity]{Excitability arises from non-linearity. The equivalent circuit depicts the sodium and potassium currents, together with $g_L$, the \emph{leakage current} through the bilayer. If all conductances were constant we'd still have a linear response; the conductances depend on the voltage, however, which gives rise to non-linearity}
	\includegraphics[width=0.7\textwidth]{excitability-arises-from-non-linearity}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption[The ion channel is an elaborate molecular machine]{The ion channel is an elaborate molecular machine. It contains a gate that prevents ions from entering, which is controlled by a sensor. The gate for the potassium subchannel contains 4 subunits, so the probability of it being open is $P_k \propto n^4$ (The lower panel in Figure \ref{fig:ion-channels} depicts the flow when all 4 gates are open, vs the flow when at least one is closed). This probability increases when depolarized.}
	\includegraphics[width=0.5\textwidth]{ion-channel-as-molecular-machine}
	\end{center}
\end{figure}
State transitions occur at voltage dependent rates. Letting
\begin{align*}
	\alpha_n(V) =& p[C \rightarrow P]\\
	\beta_n(V) =& p[O \rightarrow C] \text{ we have}\\
	\frac{dn}{dt} =& \alpha_n(V)(1-n) - \beta(V)n  \numberthis \label{eq:n}
\end{align*}
We can rewrite using time constant $\tau$.
\begin{align*}
	\tau_n(V) \frac{dn}{dt} =&n_\infty(V)-n\text{, where}\\
	\tau_n(V) =& \frac{1}{\alpha_n(V) + \beta_n(V)}\\
	n_\infty(V) =&  \frac{\alpha_n(V)}{\alpha_n(V) + \beta_n(V)}
\end{align*}

\begin{figure}[H]
	\begin{center}
		\caption[The sodium channel]{The sodium channel}
		\begin{subfigure}[t]{0.6\textwidth}
			\caption{There are 3 subunits, similar to those for potassium, plus an inactivation gate that can block the channel when open.}
			\includegraphics[width=0.6\textwidth]{na-channel1}
		\end{subfigure}
		\begin{subfigure}[t]{0.6\textwidth}
			\caption{$P_{Na}\propto m^3 h$. Voltage increases $m$ and decreases $h$, so there is a voltage window in which sodium is able to flow.}
			\includegraphics[width=0.6\textwidth]{na-channel2}
		\end{subfigure}
		\begin{subfigure}[t]{0.6\textwidth}
			\caption{As soon as sodium starts to flow, it decreases $h$: sodium is transient or self limiting. This is one of the mechanisms that switches off the spike.}
			\includegraphics[width=0.6\textwidth]{na-channel3}
		\end{subfigure}
	\end{center}
\end{figure}
\begin{itemize}
	\item \gls{gls:depolarization} increases $m$--activation
	\item \gls{gls:hyperpolarization} increases $h$--deactivation 
\end{itemize}
\begin{align*}
	\frac{dm}{dt} =& \alpha_m(V)(1-m) - \beta(V)m \numberthis \label{eq:m}\\
	\frac{dh}{dt} =& \alpha_h(V)(1-h) - \beta(V)h\numberthis \label{eq:h} 
\end{align*}
We can use these $n$, $m$, and $h$ to determine the voltage dependent conductances
\begin{align*}
	g_k(V) =& \bar{g}_k n^4\\
	g_{Na}(V)=& \bar{g}_{Na} m^3h
\end{align*}

\begin{figure}[H]
	\caption{The Hodgkin-Huxley model}\label{fig:hodgkin-huxley}
	\includegraphics[width=0.9\textwidth]{hodgkin-huxley}
\end{figure}

\begin{align*}
	V =& IR\\
	C_m\frac{dV}{dt}=&-\sum_{i}g_i(V-E_i) + I_e\numberthis\label{eq:hodgkin:huxley:precursor}\\
	-C_m\frac{dV}{dt}=&g_L(V-E_L)+\bar{g_k}n^4(V-E_K)+\bar{g_{Na}}m^3h(V-E_{Na})-I_e\numberthis\label{eq:hodgkin:huxley}
\end{align*}
Equations \eqref{eq:hodgkin:huxley}, \eqref{eq:n}, \eqref{eq:m}, and \eqref{eq:h} are known as the Hodgkin-Huxley equations \cite{hodgkin1952currents}.

\begin{figure}[H]
	\caption[Dynamics of Activation Inactivation]{Dynamics of Activation Inactivation. The right hand curves shows how quickly each variable responds to a change in voltage.}
	\includegraphics[width=0.9\textwidth]{dynamics-activation-inactivation}
\end{figure}

\begin{figure}[H]
	\caption[What makes a neuron compute?]{What makes a neuron compute? So far everything we have seen is linear. But this figure shows that something else is happening...}
	\includegraphics[width=0.9\textwidth]{what-makes-neuron-compute}
\end{figure}

\begin{figure}[H]
	\caption[Anatomy of a Spike]{Anatomy of a Spike}
	\includegraphics[width=0.9\textwidth]{anatomy-of-a-spike}
\end{figure}

\begin{figure}[H]
	\caption[Where to from here? Simplification or Realism?]{Where to from here? We can simplify, Section \ref{sec:simplified:model_neurons}, or complexify, Section \ref{sec:a:forest:of:neurons}}\label{fig:where:to_from:here}
	\includegraphics[width=0.9\textwidth]{where-to-from-here}
\end{figure}

\subsection{Simplified Model Neurons}\label{sec:simplified:model_neurons}
This section aims to explore the right hand branch of Figure \ref{fig:where:to_from:here}: can we build a simpler model that captures the relevant dynamics?

\begin{figure}[H]
	\caption[The electrical personalities of neurons]{The electrical personalities of neurons.}
	\begin{subfigure}[t]{0.9\textwidth}
		\caption{On the top, you see cortical neuron early in development.}
		\includegraphics[width=0.9\textwidth]{electrical-personalities1}
	\end{subfigure}
	\begin{subfigure}[t]{0.9\textwidth}
		\caption{Here we have thalamic neurons that have been recorded under different \glspl{gls:depolarization}. You can see a very characteristic bursting pattern, where a bunch of spikes are generated in a clump. And at a different \gls{gls:depolarization}, those bursts almost disappear, and you get single spikes, more like, more like in	the case of the cortical neuron.}
	\includegraphics[width=0.9\textwidth]{electrical-personalities2}
	\end{subfigure}
	\begin{subfigure}[t]{0.9\textwidth}
		\caption{Here's a motor neuron. You see very regular firing. Motor neurons tend to fire very regularly, and the noise leads only to small deviations in the regular timing of spikes.}
		\includegraphics[width=0.9\textwidth]{electrical-personalities3}
	\end{subfigure}
\end{figure}

\begin{figure}[H]
	\caption[Neuron, what are you trying to tell us?]{Neuron, what are you trying to tell us? So, we see that neurons can have a wide range of firing patterns, which come about partly because of the nature of their dynamics, and partly because of the nature of their inputs. Let's look at some potential examples of firing patterns.}
	\begin{subfigure}[b]{\textwidth}
		\caption{Imagine that a neuron fired regularly
			like this.}
		\includegraphics[width=0.9\textwidth]{what1}
	\end{subfigure}
	\begin{subfigure}[b]{\textwidth}
		\caption{ And to a second input, it also fires
			regularly, but with a different spiking interval.
			So one might feel comfortable thinking about this neuron's behavior as
			expressing a rate code. The spike frequency signals the input.}
		\includegraphics[width=0.9\textwidth]{what2}
	\end{subfigure}
	\begin{subfigure}[b]{\textwidth}
	\caption{What, though, if we now had this case? Here, the mean frequency is the same, but now the firing times of spikes are shifted slightly.
		So, we might imagine that these little changes in local frequency and code
		stimulus information, may be like frequency modulator or FM signals.}
	\includegraphics[width=0.9\textwidth]{what3}
	\end{subfigure}
	\begin{subfigure}[b]{\textwidth}
	\caption{In the next case here, the main firing rate might still be important.
		But there's so much variability in timing that, that suggests that precise spiked times might mean something distinct about the input.}
	\includegraphics[width=0.9\textwidth]{what4}
	\end{subfigure}
	\begin{subfigure}[b]{\textwidth}
	\caption{What about this final case? Here now you see that there are perhaps
		two distinct symbols in the code. This looks like the bursting that we saw
		in this thalamic neuron, are these single spikes signalling something different 	than these, than these groups of spikes, or these bursts.}
	\includegraphics[width=0.9\textwidth]{what5}
	\end{subfigure}
\end{figure}

So neurons are capable of firing in many different ways, and we'd like out simplified model to be able to account for this.

\begin{figure}[H]
	\caption[Capturing the Basic Dynamics of Neurons]{Capturing the Basic Dynamics of Neurons.}
	\includegraphics[width=0.9\textwidth]{capturing-basic-dynamics}
\end{figure}

\begin{align*}
	\frac{dV}{dt} =& f(V) + I(t) \text{, what if we try a linear $f$}\\
	=&-a(V-V_0) + I(t)
\end{align*}
This has a stable fixed point.

\begin{figure}[H]
	\caption[Make it spike]{Make it spike. We want to spike when the voltage reached the threshold, $V_{th}$, then reset to $V_{reset}$}\label{fig:make_it_spike}
	\includegraphics[width=0.9\textwidth]{make-it-spike}
\end{figure}

\begin{figure}[H]
	\caption[The integrate and fire neuron]{The integrate and fire neuron. This is essentially \eqref{eq:hodgkin:huxley:precursor} plus the threshold and reset behaviour of Figure \ref{fig:make_it_spike}: $V\rightarrow V_{th}$ fires spike then resets $V\rightarrow V_{reset}$.}
	\includegraphics[width=0.9\textwidth]{integrate-and-fire}
\end{figure}

\begin{figure}[H]
	\caption[How to make it excitable?]{How to make it excitable? Add an unstable fixed point.}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Use $V_{max}$ of Figure \ref{fig:make_it_spike}}
		\includegraphics[width=0.9\textwidth]{how-excitable}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Reset}
		\includegraphics[width=0.9\textwidth]{how-excitable1}
	\end{subfigure}
\end{figure}

\begin{figure}[H]
	\caption[Exponential integrate and fire]{Exponential integrate and fire\cite{fourcaud2003spike}: $f(V)=-a(V-V_0) + \exp\big[\frac{V-V_{th}}{\Delta}\big]$ Once again we have to add a fire and reset.}
	\includegraphics[width=0.9\textwidth]{exponential-integrate-and-fire}
\end{figure}

\begin{figure}[H]
	\caption[The $\theta$ neuron]{The $\theta$ neuron\cite{ermentrout1986parabolic}: $\frac{d\theta}{dt}=1-\cos\theta + (1+\cos\theta)I(t)$. There is no need for an explicit reset.}
	\includegraphics[width=0.9\textwidth]{theta-neuron}
\end{figure}

\begin{figure}[H]
	\caption[An additional fixed point]{An additional fixed pointy the new stable fixed point: we still need a reset. Now the system is trapped b}
	\includegraphics[width=0.9\textwidth]{3fp}
\end{figure}

\begin{figure}[H]
	\caption[In order to escape from the fixed point, try a 2-dimensional model]{In order to escape from the fixed point, we try a 2-dimensional model. This is inspired by the Hodgkin-Huxley model. where two things happen: the $Na$ channel switches off, and the $K$ channel switched on. This figure shows the nullclines.}
	\includegraphics[width=0.9\textwidth]{2d}
\end{figure}

\begin{align*}
	\frac{dV}{dt} =& F(V) + G(U) +I(t)\\
	\frac{dU}{dt} =& -U + H(V)
\end{align*}

\begin{figure}[H]
	\caption[Trajectory of 2d spiking  model]{Trajectory of  2d spiking model}
	\includegraphics[width=0.9\textwidth]{2d-spike}
\end{figure}

Figure \ref{fig:simple:model} depicts the  Simple Model \cite{izhikevich2003simple} specified by \eqref{eq:simple1} and \eqref{eq:simple2}.
\begin{align*}
	\frac{dV}{dt} =& \alpha V + \beta V^2 - u +I(t)\numberthis \label{eq:simple1}\\
	\frac{du}{dt} =& a(bV-u)\numberthis \label{eq:simple2}
\end{align*}

\begin{figure}[H]
	\caption[The Simple Model]{The Simple Model\cite{izhikevich2003simple}. Electronic version of the figure and reproduction permissions are freely available at \url{www.izhikevich.com}}\label{fig:simple:model}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Phase Plane}
		\includegraphics[width=0.9\textwidth]{simple-model}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Example}
		\includegraphics[width=0.9\textwidth]{simple1}
	\end{subfigure}
	\begin{subfigure}[b]{0.9\textwidth}
		\caption{Model fits to real neurons}
		\includegraphics[width=0.9\textwidth]{simple2}
	\end{subfigure}
\end{figure}


\subsection{A Forest of Dendrites}\label{sec:a:forest:of:neurons}
This section returns to the left hand branch of Figure \ref{fig:where:to_from:here}.
\begin{figure}[H]
	\caption[Neurons have complicated spatial structures]{Neurons have complicated spatial structures: what is the appropriate
		level of description of a single neuron that's necessary to understand brain
		operation? Voltage decays with distance in passive membranes.}
	\includegraphics[width=0.8\textwidth]{neurons-have-complicated-spatial-structures}
\end{figure}

\begin{figure}[H]
	\caption[Do dendrites feel what is going on in soma?]{Do dendrites feel what is going on in soma?}
	\includegraphics[width=0.8\textwidth]{do-dendrites-feel-what-is-going-on-in-soma}
\end{figure}

 Furthermore, how thin the dendrite is
affects how big a voltage change you could make with a given amount of current
input. The thinner the dendrite the larger the
voltage change but generally the further away the the more that input gets
filtered and attenuated. This tells us the inputs that come along
different parts of the dendrite can have very different effects and very different
influence on firing at the soma. As you can image this can have a
tremendous impact on the information that is integrated and representated by the
receiving neurons

\begin{figure}[H]
	\caption[The theoretical basis for understanding voltage propagation]{The theoretical basis for understanding voltage propagation in dendrites and axons is cable theory. The voltage, V, is now a function of both space and time; $r_m$ is the resistance across the membrane, and $r_i$ the axial resistance along a short section of the cylinder\cite{Johnston1994}}
	\includegraphics[width=0.8\textwidth]{cable-theory}
\end{figure}

\begin{align*}
	\frac{1}{r_i}\frac{\partial^2V_m(x,t)}{\partial x^2} =& c_m \frac{\partial V}{\partial t} + \frac{V_m}{r_m}\text{, or} \\
	\lambda^2\frac{\partial^2V_m(x,t)}{\partial x^2} =& \tau_m \frac{\partial V}{\partial t} + V_m \text{, where}\\
	\tau_m =& r_m c_m \text{, time constant, and}\\
	\lambda =& \sqrt{\frac{r_m}{r_i}} \text{, space constant.}
\end{align*}


\begin{figure}[H]
	\caption[How does voltage decay over space and time?]{How does voltage decay over space and time? We assume an infinite cable, with constant current at $x=0$. Potential decays over a scale comparable to the space constant: $V(x)\propto \exp(-\frac{x}{\lambda})$.}
	\includegraphics[width=0.8\textwidth]{potential-decay-space}
\end{figure}


\begin{figure}[H]
	\caption[How does pulse decay over space and time?]{How does pulse decay over space and time? Assume an infinite cable, with a current pulse at $t=0, x=0$: potential peaks later, and at lower values, for points further away from the input.}
	\includegraphics[width=0.8\textwidth]{voltage-decay-space-time}
\end{figure}

The general solution is:
\begin{align*}
	V(x,t) \propto \sqrt{\frac{\tau}{4 \pi \lambda^2 t}} \exp \big[\underbrace{-\frac{t}{\tau}}_\text{Exponential decay} -\underbrace{\frac{\tau x^2}{4 \lambda^2 t}}_\text{Diffusive spread}\big]
\end{align*}

In theory we can use the Green's function to compute the response to any input, but the geometry can become very complicated, and many dendrites are active, i.e. they have ion channels. The path forward is to divide the dendritic arbor into compartments.

\begin{figure}[H]
	\caption[Compartmental models.]{Compartmental models. Each compartment is one $\frac{dV}{dt}$ equation, usually with no dependence on $x$.}
	\includegraphics[width=0.8\textwidth]{compartmental-models}
\end{figure}


\begin{figure}[H]
	\caption[Rall model for passive dendrites]{Rall model for passive dendrites. If \eqref{eq:rall} is satisfied, impedances match and can replace each pair of branches with a single cable segment with equivalent surface area and \gls{gls:electrotonic}  length.} 
	\includegraphics[width=0.8\textwidth]{rall-model}
\end{figure}
It turns out that the condition on diameters is often correct for real networks.
\begin{align*}
	d_{11}^2 + 	d_{12}^2 =& 	d_{1}^2 \numberthis \label{eq:rall}
\end{align*}

\begin{figure}[H]
	\caption[Now including active properties (channels).]{Now including active properties (channels). The Rall model down not handle ion channels: furthermore the density of ion channels often varies along dendrites.}
	\includegraphics[width=0.8\textwidth]{full-model}
\end{figure}


\begin{figure}[H]
	\caption[What do dendrites add to neural computation?\cite{london2005dendritic}]{What do dendrites add to neural computation? In the hippocampus, dendrites perform preprocessing so that all signals reaching the soma have a similar shape, no matter where the originated (somatic scaling).}
	\includegraphics[width=0.8\textwidth]{what-do-dendrites-add}
\end{figure}


\begin{figure}[H]
	\caption[Delay lines in sound localization]{Delay lines in sound localization}
	\includegraphics[width=0.8\textwidth]{delay-sound}
\end{figure}


\begin{figure}[H]
	\caption[Direction selectivity in the retina]{Direction selectivity in the retina}
	\includegraphics[width=0.8\textwidth]{detection-selectivity}
\end{figure}


\section{Computing with Networks}\label{sec:week6}
Highlights of our journey thus far
\begin{itemize}
	\item Neuroscience Review
	\begin{itemize}
		\item Neurons, synapses, and brain regions
	\end{itemize}
	\item Neural Encoding
	\begin{itemize}
		\item What makes a neuron fire? (STA, covariance analysis)
		\item Poisson model of spiking
	\end{itemize}
	\item Neural Decoding and Information Theory
	\begin{itemize}
		\item Stimulus discrimination and signal detection
		\item Population decoding and Bayesian estimation
		\item Information and neural coding principles
	\end{itemize}
	\item Single Neuron Models
	\begin{itemize}
		\item RC circuit model of membrane
		\item Hodgkin-Huxley and compartmental models
		\item Integrate-and-fire and simplified neuron models
	\end{itemize}
\end{itemize}

This leads to the question: how do neurons connect to form networks? The use synapses--see Figure \ref{fig:enter:the:synapse} and the next Section.

\subsection{Making Connections between Neurons}

\begin{figure}[H]
	\caption[What do synapses do?]{What do synapses do? The spike causes some chemicals to be introduced into the synaptic cleft; these are going to bind with some receptors, which will increase or decrease postsynaptic membrane potential.}
	\includegraphics[width=0.8\textwidth]{what-do-synspses-do}
\end{figure}

\begin{itemize}
	\item An excitatory synapse
	\begin{itemize}
		\item Input spike 
		\item Neurotransmitter release 	(e.g., Glutamate) 
		\item Binds to receptors
		\item Ion channels open
		\item positive ions (e.g. $Na^+$) enter cell
		\item Depolarization (increases local membrane potential)
	\end{itemize}
	\item An inhibitory synapse
	\begin{itemize}
		\item Input spike
		\item Neurotransmitter 	release (e.g., GABA)
		\item  Binds to receptors
		\item  Ion channels open
		\item  negative ions (e.g. $Cl^-$ ) enter cell, or positive ions (e.g.,$K^+$) leave cell 
		\item 	Hyperpolarization 	(decreases local membrane potential)
	\end{itemize}
\end{itemize}



\begin{figure}[H]
	\caption[Construct a computational model of the effects of a synapse.]{How do we construct a computational model of the effects of a synapse on the membrane potential?}
	\includegraphics[width=0.8\textwidth]{want-computational-model}
\end{figure}

\begin{figure}[H]
	\caption[Recall the RC circuit model of the cell]{Recall the RC circuit model of the cell. \cite{dayan2005theoretical}}
	\begin{subfigure}[t]{0.6\textwidth}
		\caption{We model the membrane as a resistance and a capacitor}
		\includegraphics[width=\textwidth]{rc-circuit-model}
	\end{subfigure}
	\begin{subfigure}[t]{0.3\textwidth} 
		\caption{We inject a current $I_e$: \eqref{eq:rc:equivalent} tells us thatthe voltage converges to the steady state $V_{SS}=E_L+I_eR_m$. If we then turn of the current, the voltage returns to $E_L$}
		\includegraphics[width=0.9\textwidth]{voltage-from-rc-model}
	\end{subfigure}
\end{figure}

\begin{align*}
	c_m  \approx& 10 \; nF/mm^2 \text{, membrane capacitance}\\
	r_m \approx& 1 \; M \Omega \; mm^2 \text{, membrane resistance}\\
	C_m =& c_m A \text{, where A is the area of the cross-section}\\
	R_m =& \frac{r_M}{A} \text{. Using definition of capacitance:}\\
	q =& C_m V \text{. Differentiating to get the current:}\\
	c_m \frac{dV}{dt} =&-\frac{V-E_L}{r_m} + \frac{i_e}{A} \text{, or, equivalently}\\
	\tau_m \frac{dV}{dt} =& -(V-E_L) + I_e r_m \text{, where} \numberthis \label{eq:rc:equivalent}\\
	\tau_m =& r_m c_m = R_m c_m \text{, defines the time constant}
\end{align*}

\begin{figure}[H]
	\caption[Modeling Synaptic Inputs]{Modeling Synaptic Inputs. Synapses release neurotransmitters that  ion channels to open and close, and that, in turn, causes the membrane potential to change. How do we model opening anc closing channels?}
	\includegraphics[width=0.9\textwidth]{how-to-model-synapse}
\end{figure}

\begin{figure}[H]
	\caption[Reprise the Hodgkin-Huxley model]{Let's reprise the Hodgkin-Huxley model, and ask whether we can model the channels similarly for synapses.}
	\includegraphics[width=0.9\textwidth]{hodgkin-huxley-reprise}
\end{figure}

\begin{align*}
	\tau_m \frac{dV}{dt} =& -i_m r_m + I_e R_m\\
	i_m =& \frac{1}{r_m} (V-E_L) + g_{K,max} n^4 (V-E_K) + g_{Na,max}m^3 h (V-E_{Na})\\
	E_L =& -54 mV,\; E_K = -77mV,\; E_{Na} = + 50mV
\end{align*}

\begin{figure}[H]
	\caption[Modeling synapse using synaptic conductance]{Modeling synapse using synaptic conductance.}
	\includegraphics[width=0.9\textwidth]{hh-synapse}
\end{figure}

\begin{align*}
	\tau_m \frac{dV}{dt} =& -[(V-E_L)+g_s(V-E_S)r_m] + I_e R_m \text{, where}\\
	g_s =& \text{ synaptic conductance}\\
	e_s =& \text{ equilibrium voltage of synapse.}
\end{align*}
We write:
\begin{align*}
		g_s =& g_{s,max}P_{rel}P_{s} \text{, where}\\
		P_{rel}=& \text{Probability of transmitter release given an input spike}\\
		P_{s}=& \text{Probability of postsynaptic channel opening given release}
\end{align*}

Let's construct a basic model.
\begin{itemize}
	\item Assume $P_{rel}=1$
	\item Model the effect of a single spike on $P_s$
	\item Kinetic model
\end{itemize}

\begin{align*}
	P(Closed\rightarrow Open)=&\alpha_s\\
	P(Open\rightarrow Closed)=&\beta_s\\
	\frac{dP_s}{dt}=&\alpha_s(1-P_s)-\beta_s P_s
\end{align*}

\begin{figure}[H]
	\caption{What does $P_s$ look like over time, given a spike?}
	\begin{subfigure}[t]{0.45\textwidth}
		\caption{Exponential function gives a reasonable fit for some synapses, e.g. AMPA, but not $GABA_A$.}
		\includegraphics[width=0.9\textwidth]{PS1}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\caption{... or NMDA}
		\includegraphics[width=0.9\textwidth]{PS2}
	\end{subfigure}
	\begin{subfigure}[t]{0.9\textwidth}
		\caption{$GABA_A$ and NMDA can be fit using Alpha function: $\alpha(t)=\frac{t}{\tau_{peak}} exp\big(1-\frac{t}{\tau_{peak}}\big)$}
		\includegraphics[width=0.9\textwidth]{PS3}
	\end{subfigure}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption[What if we have more than one spike?]{What if we haveel more than one spike? The Linear Filter Model of a Synapse is useful.}\label{fig:LinearFilter}
		\includegraphics[width=0.9\textwidth]{LinearFilter}
	\end{center}
\end{figure}

We characterize the spikes by the response function $\rho_i$
\begin{align*}
	\rho_b(t) =& \sum_{i} \delta(t-t_i) \text{, we want the synaptic conductance at $b$}\\
	g_b(t) =& g_{b,max}\sum_{t_i<t} K(t-t_i) \text{, K() is the filter for synapse $b$}\\
	=& g_{b,max}\int_{-\infty}^{t} K(t-\tau) \rho_b(\tau) d\tau \numberthis \label{eq:LinearFilter}
\end{align*}

\begin{figure}[H]
	\begin{center}
		\caption[Network of Integrate and Fire]{Network of Integrate and Fire. }
		\begin{subfigure}[t]{0.45\textwidth}
			\caption{Excitatory synapses, Alpha function: Alternate firing}
			\includegraphics[width=0.9\textwidth]{net-excitory}
		\end{subfigure}
		\begin{subfigure}[t]{0.45\textwidth}
			\caption{Inhibitory synapses, Alpha function: Synchronized firing}
			\includegraphics[width=0.9\textwidth]{net-inhibitory}
		\end{subfigure}
	\end{center}
\end{figure}

\subsection{Introduction to Network Models}

There  two options for modeling networks: 
\begin{enumerate}
	\item  Model networks using Spiking neurons
	\begin{itemize}
		\item Advantages: Model computation and learning based on:
		\begin{itemize}
			\item Spike Timing
			\item Spike Correlations/Synchrony between neurons
		\end{itemize}
		\item Disadvantages: Computationally expensive
	\end{itemize}
	\item Use neurons with firing-rate outputs (real valued outputs)
	\begin{itemize}
		\item Advantages: Greater efficiency, scales well to large networks
		\item Disadvantages: Ignores spike timing issues
	\end{itemize}
\end{enumerate}

Question: How are these two approaches related?
Recall the linear filter--Figure \ref{eq:LinearFilter} and \eqref{eq:LinearFilter}.

\begin{figure}[H]
	\begin{center}
		\caption{Constructing a Model}
		\begin{subfigure}[t]{0.45\textwidth}
			\caption{From a single synapse to multiple synapses.}\label{fig:from-single-synapse-to-multiple}
			\includegraphics[width=0.9\textwidth]{from-single-synapse-to-multiple}
		\end{subfigure}
		\begin{subfigure}[t]{0.45\textwidth}
			\caption{From spiking to firing.}\label{fig:from-spiking-to-firing}
			\includegraphics[width=0.9\textwidth]{from-spiking-to-firing}
		\end{subfigure}
	\end{center}
\end{figure}

Total synaptic current is given by:
\begin{align*}
	I_s(t) =& \sum_{b=1}^{n} I_b(t)\\
	=&  \sum_{b=1}^{n} w_b \int_{-\infty}^{t} K(t-\tau) \rho_b(\tau) d\tau \text{, from Figure \ref{fig:from-single-synapse-to-multiple}}\\
	\approx & \sum_{b=1}^{n} w_b \int_{-\infty}^{t} K(t-\tau) u_b(\tau) d\tau \text{, from Figure \ref{fig:from-spiking-to-firing}} \numberthis \label{eq:total:synaptic:current}
\end{align*}

We have replaced the spike train $\rho_i(t)$ with the firing rate $u_i(t)$. This could fail if there are correlations between the input neurons, or when there is synchrony, but in many cases the replacment is OK.

Now suppose thate we can write:
\begin{align*}
	K(t) =& \frac{1}{\tau_s} \exp \big(-\frac{t}{\tau_s}\big)\text{. Differentiating \eqref{eq:total:synaptic:current}:}\\
	\tau_s \frac{dI_s}{dt} =& -I_s + \sum_b w_b u_b\\
	=&  -I_s +\vec{w} \cdot \vec{u}
\end{align*}

\begin{figure}[H]
	\begin{center}
		\caption{Firing rate based network model.}\label{fig:firing-rate-based-network-model}
		\includegraphics[width=0.9\textwidth]{firing-rate-based-network-model}
	\end{center}
\end{figure}

\begin{align*}
	\tau_r \frac{dv}{dt} =& -v + F(I_s(t)) \numberthis \label{eq:fig:firing-rate-based-network-model1} \\ 
	\tau_s \frac{dI_s}{dt} =&  -I_s +\vec{w} \cdot \vec{u}
\end{align*}

If $\tau_s \ll \tau_r$ \eqref{eq:fig:firing-rate-based-network-model1} becomes:
\begin{align*}
	\tau_r \frac{dv}{dt} =& -v + F(\vec{w} \cdot \vec{u})
\end{align*}

If $\tau_r \ll \tau_s$ \eqref{eq:fig:firing-rate-based-network-model1} becomes:
\begin{align*}
	v =&  F(I_s(t))
\end{align*}

Finally, the steady state case is:
\begin{align*}
	v_{SS} =& F(\vec{w} \cdot \vec{u}) \text{, which is used with Artificial Neural Networks}
\end{align*}

\begin{figure}[H]
	\begin{center}
		\caption{Multiple Outputs}
		\includegraphics[width=0.9\textwidth]{multiple-outputs}
	\end{center}
\end{figure}

\begin{align*}
	\tau\frac{d\vec{v}}{dt} =& -\vec{v} + F(W \vec{u}) \text{, where}\\
	W_{ij} =& \text{ synaptic weight from neuron $j$ to neuron $i$.}
\end{align*}

\begin{figure}[H]
	\begin{center}
		\caption{Feedforward vs recurrent networks}
		\includegraphics[width=0.9\textwidth]{feedforward-vs-recurrent}
	\end{center}
\end{figure}

\begin{align*}
	\tau\frac{d\vec{v}}{dt} =& -\vec{v} + F(W \vec{u} + \underbrace{M \vec{v}}_\text{$M=0$ for feedforward})
\end{align*}

V1 calculates derivatives.

\subsection{Recurrent Networks}

\subsubsection{Linear Recurrent Networks}

\begin{figure}[H]
	\begin{center}
		\caption{What can a linear recurrent network do?}
		\includegraphics[width=0.9\textwidth]{linear-recurrent-network}
	\end{center}
\end{figure}

\begin{align*}
	\tau \frac{d\vec{v}}{dt} =& -\vec{v} + W\vec{u} +m \vec{v}\\
	=& -\vec{v} +\vec{h} +m \vec{v} \text{, say} \numberthis \label{eq:recurrent:network}
\end{align*}

How does $\vec{v}$ behave for different $M$? We will suppose that $M$ is symmetric $N\times N$, and use eigenvectors. Then $M$ has N orthonormal eigenvectors $\vec{e}_i$ and $N$ eigenvalues $\lambda_i$.
\begin{align*}
	M \vec{e}_i =& \lambda_i \vec{e}_i \text{. We can write}\\
	\vec{v}(t) =& \sum_{i} c_i(t) \vec{e}_i \numberthis \label{eq:ev:expansion}
\end{align*}

\begin{thm}
	The solution to \eqref{eq:recurrent:network} is given by \eqref{eq:ev:expansion}.
	\begin{align*}
			c_i(t) =& \frac{\vec{h}\cdot\vec{e}_i}{1-\lambda_i}\big[1-\exp\big(\frac{-t(1-\lambda_i)}{\tau}\big)\big] + c_i(0) \exp\big(\frac{-t(1-\lambda_i)}{\tau}\big)
	\end{align*}
\end{thm}

\begin{proof}
	TBP
\end{proof}

\begin{cor}
	If any $\lambda_i>1$, network is unstable.
\end{cor}

\begin{cor}
	If all $\lambda_i<1$, network converges to the steady state value:
	\begin{align*}
		\vec{v}_{SS}=&\sum_{i}\frac{\vec{h}\cdot\vec{e}_i}{1-\lambda_i}\vec{e}_i
	\end{align*}
\end{cor}

\begin{cor}
	If all $\lambda_i<1$, and one of them, say $\lambda_1$ has the property $\lambda_i<\lambda_1$ for $i>1$
	\begin{align*}
		\vec{v}_{SS}\approx\frac{\vec{h}\cdot\vec{e}_1}{1-\lambda_1}vec{e}_1
	\end{align*}
\end{cor}

\subsubsection{Nonlinear Recurrent Networks}

\begin{align*}
	\tau \frac{d\vec(v)}{dt} =& -\vec{u} +F(\vec{h} +m \vec{v} )\text{e.g. rectification non-linearity}\\
	F(x)=&max(x,0)
\end{align*}

\section{Networks that Learn}\label{sec:week7}

\subsection{Synaptic Plasticity and Statistical Learning}\label{sec:week7:1}

\subsubsection{Hebb's Rule}

\begin{figure}[H]
	\begin{center}
		\caption{Formalizing Hebb's rule }
		\begin{subfigure}[b]{0.4\textwidth}
			\caption{If neuron A repeatedly takes part in firing neuron B, then the synapse from A to B is strengthened.}
			\includegraphics[width=0.9\textwidth]{hebbian-plasticity}
		\end{subfigure}
		\begin{subfigure}[b]{0.44\textwidth}
			\caption{Consider a single linear neuron with steady state output $v=\vec{w}\cdot\vec{u}$}
			\includegraphics[width=0.9\textwidth]{formalizing-hebbs-rule}
		\end{subfigure}
	\end{center}
\end{figure}

The basic Hebb rule is:
\begin{align*}
	\tau_w \frac{d\vec{w}}{dt} =& \vec{u} v\text{, or, discretizing} \numberthis \label{eq:hebbs:rule}\\
	w_{i+1} =& w_i + \epsilon  \vec{u} v \text{. The average effect of the rule is}\\
	\tau_w \frac{d\vec{w}}{dt} =& \langle\vec{u} v\big>_u\\
	 =& \langle\vec{u} \vec{u}^Tw\rangle_u\\
	 =& \langle\vec{u} \vec{u}^T\rangle_u w\\
	 =& Qw \text{, where $Q$ is the input \gls{gls:correlation:matrix}}\\
	 Q \triangleq& \langle\vec{u} \vec{u}^T\rangle_u
\end{align*}

\subsubsection{The Covariance Rule}
\begin{itemize}
	\item The Hebb rule only increases synaptic weights --\gls{gls:LTP}.
	\item  What about \gls{gls:LTD}, which decreases synaptic weights?
	\item  The covariance rule aims to account for both \gls{gls:LTP} and \gls{gls:LTD}.
\end{itemize}

\begin{align*}
	\tau_w \frac{d\vec{w}}{dt} =& \vec{u}\big(v-<v>\big) \text{. On average} \numberthis \label{eq:covariance:rule}\\
	\tau_w \frac{d\vec{w}}{dt} =& \langle \vec{u}\big(v-<v>\big) \rangle_u\\
	=& \langle \vec{u}\big(\vec{u}^T-<\vec{u}>^T\big) \vec{w}\rangle_u\\
	=& \Big( \langle \vec{u}^T \vec{u}\rangle - \langle \vec{u}^T \rangle \langle\vec{u}\rangle\Big)\vec{w} \\
	=& \mathbf{C} \vec{w} \text{, where $\mathbf{C}$ is the \Gls{gls:covariance:matrix}}\\
	\mathbf{C}\triangleq& \Big( \langle \vec{u}^T \vec{u}\rangle - \langle \vec{u}^T \rangle \langle\vec{u}\rangle\Big)
\end{align*}

\subsubsection{Are these learning rules stable?}

\begin{thm}
	Hebb's rule is unstable.
\end{thm}

\begin{proof}
	Differentiating \eqref{eq:hebbs:rule}:
	\begin{align*}
		\frac{d\lvert\lvert\vec{w}\rvert\rvert^2}{dt} =& 2 \vec{w} \frac{d\vec{w}}{dt}\\
		=& 2\vec{w}^T\frac{\vec{u} v}{\tau_w}\\
		=& \frac{2v}{\tau_w} \vec{w} \cdot \vec{u}\\
		=& \frac{2v^2}{\tau_w}\\
		>0
	\end{align*}
\end{proof}

\begin{thm}
	The covariance rule is unstable
\end{thm}

\begin{proof}
	Differentiating \eqref{eq:covariance:rule}
	\begin{align*}
		\frac{d\lvert\lvert\vec{w}\rvert\rvert^2}{dt} =& 2 \vec{w} \frac{d\vec{w}}{dt}\\
		=& 2 \vec{w}^T \frac{\vec{u}(v-\langle v \rangle)}{\tau_w}\\
		=& \frac{2}{\tau_w} (v^2-v\langle v \rangle)\text{. Averaging}\\
		\frac{d\lvert\lvert\vec{w}\rvert\rvert^2}{dt} =& \frac{2}{\tau_w}(\langle v^2 \rangle - \langle v \rangle^2)\\
		>& 0
	\end{align*}
\end{proof}

\begin{thm}[Oja's rule]
	The rule
	\begin{align*}
		\tau_w \frac{d\vec{w}}{dt} =& \vec{u} v - \alpha v^2 \vec{w} \text{, where $\alpha>0$} \numberthis \label{eq:oja}
	\end{align*}
is stable
\end{thm}

\begin{proof}
	Differentiating \eqref{eq:oja}
	\begin{align*}
		\frac{d\lvert\lvert\vec{w}\rvert\rvert^2}{dt} =& 2 \vec{w} \frac{d\vec{w}}{dt}\\
		=& \frac{2}{\tau_w} \vec{w}^T\big(\vec{u}v - \alpha v^2 \vec{w}\big)\\
		=& \frac{2}{\tau_w} \big( v^2 - \alpha v^2 \vec{w}^T\vec{w}\big)\\
		=& \frac{2}{\tau_w} v^2 \big(1 - \alpha\lvert\lvert\vec{w}\rvert\rvert^2\big)\text{, which converges to a steady state}\\
		\lvert\vec{w}\rvert =& \frac{1}{\alpha}
	\end{align*}
\end{proof}

\subsubsection{What does Hebbian Learning do?}

\begin{figure}[H]
	\caption[Hebbian Learning implements Principal Component Analysis]{Hebbian Learning implements Principal Component Analysis\cite{dayan2005theoretical}. It learns a weight vector aligned with the principal eigenvector of input correlation/covariance matrix, i.e. the direction of maximum variance. In each sub-figure, after training, the vector of synaptic weights was aligned parallel to the solid line}\label{fig:hebb-pca}
	\begin{subfigure}[b]{0.3\textwidth}
		\caption{The filled circles show the inputs u = ( u 1 , u 2 ) used during a training period while a Hebbian plasticity rule was applied.}\label{fig:hebb-pca1}
		\includegraphics[width=\textwidth]{hebb-pca1}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\caption{Input vectors were generated as in (\subref{fig:hebb-pca1}) except that the distribution was shifted to produce an average value $\langle u \rangle=(2, 2$).}\label{fig:hebb-pca2}
		\includegraphics[width=0.9\textwidth]{hebb-pca2}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\begin{center}
			\caption{Points from the same distribution as in (\subref{fig:hebb-pca2}) with a covariance-based Hebbian rule.}
			\includegraphics[width=0.9\textwidth]{hebb-pca3}
		\end{center}
	\end{subfigure}
\end{figure}

In Figure \ref{fig:hebb-pca}, Hebbian learning reduces the dimensionality of the data.

\begin{figure}[H]
	\caption[\gls{gls:PCA} does not correctly describe this data]{\gls{gls:PCA} does not correctly describe this data: we have lost the clustering!}
	\includegraphics[width=\textwidth]{pca-does-not-correctly-describe-this-data}
\end{figure}


\subsection{Introduction to Unsupervised Learning}

\subsubsection{Can a network learn to represent clusters?}
\begin{figure}[H]
	\caption{Can a network learn to represent clusters?}
	\begin{subfigure}[t]{0.45\textwidth}
		\caption{Raw Data}
		\includegraphics[width=\textwidth]{clustered-data}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\caption{Using neurons to represent clusters. The $\vec{w}$ are the centres of the clusters. The most active neuron is the one whose centre is closest to an input.}
		\includegraphics[width=\textwidth]{neurons-clusters1}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\caption{$v_i=\vec{w_i}\cdot\vec{u}=\vec{w_i}^T\vec{u}=\vec{u}^T\vec{w_i}$}
		\includegraphics[width=\textwidth]{neurons-clusters2}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\caption{Competitive Learning. Given a new input, pick the most active neuron, i.e. the one whose weights are closest to the input, and update weights using \eqref{eq:competitive:learning}.}\label{fig:competitive:learning}
		\includegraphics[width=\textwidth]{competitive-learning}
	\end{subfigure}
\end{figure}

If Figure \ref{fig:competitive:learning} 
\begin{align*}
	\Delta \vec{w}=&\epsilon(\vec{u_t}=\vec{w}) \text{, where} \numberthis \label{eq:competitive:learning}\\
	\epsilon=&\frac{1}{t} \text{, to use a running average of inputs, or}\\
	\epsilon=&\text{ a small value, to adapt to new inputs for an indefinite period of time.}
\end{align*}

\begin{figure}[H]
	\begin{center}
		\caption[Competitive Learning and Self Organizing Maps]{Competitive Learning and Self Organizing Maps (a.k.a. Kohonen Maps).}
		\begin{subfigure}[t]{0.9\textwidth}
			\caption{Given an input, pick the winning neuron; update weights for that neuron and other neurons in the neighbourhood of the winner. The the primary visual cortex appears to use a similar  scheme--(\subref{fig:kohonen-maps-v1})}
			\includegraphics[width=\textwidth]{kohonen-maps}
		\end{subfigure}
		\begin{subfigure}[t]{0.9\textwidth}
			\caption{Orientation preference map in the primary visual cortex (V1).}\label{fig:kohonen-maps-v1}
			\includegraphics[width=0.6\textwidth]{kohonen-maps-v1}
		\end{subfigure}
	\end{center}
\end{figure}

\subsubsection{Is there a principled way of learning models of input data?}
\begin{figure}[H]
	\caption{Unsupervised Learning via a Generative Model}
	\begin{subfigure}[t]{0.4\textwidth}
		\caption{The Generative Model}
		\includegraphics[width=\textwidth]{unsupervised-learning-via-generative-model}
	\end{subfigure}
	\begin{subfigure}[t]{0.5\textwidth}
		\caption{Example: Gaussian Mixture Model}\label{fig:gmm}
		\includegraphics[width=\textwidth]{unsupervised-learning-example}
	\end{subfigure}
\end{figure}

We use the Expectation Maximization Algorithm, iterating the two following steps until convergence:
\begin{enumerate}
	\item E-step: compute posterior distribution of $v$, e.g. cluster assignments.
	\begin{align*}
		p(v\vert u;\mathbf{G})=&\frac{p(u\vert v ;\mathbf{G}) p(v;\mathbf{G})}{p(u;\mathbf{G})}\\
		=&\frac{N(\vec{u};\vec{\mu_v},\sigma_v I).\gamma_v}{\sum_v N(\vec{u};\vec{\mu_v},\sigma_v I).\gamma_v}
	\end{align*}
	\item M-step: change parameters $\mathbf{G}$ using results from E-step.
	\begin{align*}
		\vec{\mu_v} =&\frac{\sum_u P[v\vert \vec{u};\mathbf{G}]\vec{u}}{\sum_u P[v\vert \vec{u};\mathbf{G}]}\\
		\vec{\sigma_v} =&\frac{\sum_u P[v\vert \vec{u};\mathbf{G}]\lvert \vec{u}-\vec{\mu_v}\rvert^2 }{\sum_u P[v\vert \vec{u};\mathbf{G}]}\\
		\vec{\gamma_v} =&\frac{\sum_u P[v\vert \vec{u};\mathbf{G}]}{N_u}
	\end{align*}
\end{enumerate}

\begin{figure}[H]
	\begin{center}
		\caption{Expectation-Maximization--an example}
		\includegraphics[width=\textwidth]{em-results}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption{Suppose data are natural images}\label{fig:suppose-data-are-natural-images}
		\includegraphics[width=\textwidth]{suppose-data-are-natural-images}
	\end{center}
\end{figure}

\begin{itemize}
	\item What kind of generative model would you use?
	\item How do you learn the “causes” of such images
\end{itemize}

\subsection{Sparse Coding \& Predictive Coding}

Returning to Figure \ref{fig:suppose-data-are-natural-images}, we ask:
\begin{itemize}
	\item Can we learn a good representation of natural images?
	\item What does the brain do?
\end{itemize}

\begin{figure}[H]
	\begin{center}
		\caption[Eigenfaces]{Eigenfaces\cite{turk1991eigenfaces}}
		\begin{subfigure}[t]{0.9\textwidth}
			\caption{Face images}
			\includegraphics[width=\textwidth]{eigenfaces1}
		\end{subfigure}
		\begin{subfigure}[t]{0.9\textwidth}
			\caption{Eigenvectors of the input covariance matrix}
			\includegraphics[width=\textwidth]{eigenfaces2}
		\end{subfigure}
		\begin{subfigure}[t]{0.9\textwidth}
			\caption{A linear model of images using eigenvectors}
			\includegraphics[width=\textwidth]{eigenfaces3}
		\end{subfigure}
	\end{center}
\end{figure}

We can expand in principal components
\begin{align*}
	\vec{u} =& \sum_{i=1}^{N} \vec{e_i} v_i \\
	=& \sum_{i=1}^{M} \vec{e_i} v_i + \vec{noise} \text{, where $M\ll N$}
\end{align*}

Eigenvector representation is good for compression but not so good if you want to extract the local components (or parts) of an image (e.g., parts of a face, local edges in a scene, etc.)

\begin{figure}[H]
	\begin{center}
		\caption{Can we salvage the idea of a linear model for images?}
		\includegraphics[width=\textwidth]{linear-model-images}
	\end{center}
\end{figure}

\begin{align*}
	\vec{u} =& \sum_{i=1}^{M} \vec{g}_i v_i + \vec{noise}\\
	=& \mathbf{G} \vec{v} + \vec{noise}
\end{align*} 

We will allow the number of features, $M$, to be larger than $N$, then number of pixels.

If the noise is Gaussian white noise, the likelihood is given by:

\begin{align*}
	p[\vec{u}\vert \vec{v};\mathbf{G}] =& Gaussian(u;\mathbf{G}v,I)\\
	\propto& exp\big(-\frac{\lvert\lvert \vec{u}-\mathbf{G} \vec{v}\rvert rvert^2}{2}\big)\text{, whence}\\
	\log p[\vec{u}\vert \vec{v};\mathbf{G}] =& -\frac{\lvert\lvert \vec{u}-\mathbf{G} \vec{v}\rvert rvert^2}{2} + C
\end{align*}

So minimizing the reconstruction error is the same as maximizing likelihood.

We will assume that the causes are independent, for simplicity, so $P[\vec{v}]$ is product of probabilities of the original causes.

\begin{align*}
	p[\vec{v]} =& \prod_{i} p[v_i]\\
	\ln p[\vec{v]} =& \sum_{i} \log p[v_i]
\end{align*}

For any input, we want only a few causes $v_i$ to be active
\begin{itemize}
	\item v i = 0 most of the time but high for some inputs
	\item Suggests sparse distribution for $p[v_i]$: peak at 0 but with heavy tail (also called super- Gaussian distribution)
\end{itemize}

\begin{figure}[H]
	\begin{center}
		\caption{Examples of sparse priors}
		\includegraphics[width=\textwidth]{sparse-priors}
	\end{center}
\end{figure}

\subsubsection{Bayesian approach to finding v and learning G}

\begin{itemize}
	\item Find v and G that maximize posterior probability of causes
	\begin{align*}
		p[\vec{v}\vert\vec{u};G] =& Z p[\vec{u}\vert\vec{v};G] p[\vec{v};G] \text{, where $Z$ is the partition function}
	\end{align*}
	\item Equivalently, maximize log posterior, which entails minimizing the reconstruction error while maximizing the sparseness. 
	\begin{align*}
		F(\vec{v},G) =& \log p[\vec{u}\vert\vec{v};G] + \log p[\vec{v};G] + log Z\\
		=& \underbrace{-\frac{1}{2} \lvert \lvert \vec{u} -\vec{v} \rvert \rvert^2}_\text{Reconstruction error} + \underbrace{\sum_i g_i(v_i)}_\text{sparseness constraint} + K
	\end{align*}
	\item Alternate between two steps
	(similar to EM algorithm):
	\begin{itemize}
		\item 	Maximize F with respect to $\vec{v}$, 	keeping G fixed
		\item Maximize F with respect to G, given the  $\vec{v}$ from above
	\end{itemize}
\end{itemize}


\begin{figure}[H]
	\begin{center}
		\caption[One way to maximize F with respect to $\vec{v}$ is Gradient Ascent]{One way to maximize F with respect to $\vec{v}$ is gradient ascent $\frac{d\vec{v}}{dt} \propto \frac{\partial F}{\partial\vec{v}}$, which moves $\vec{v}$ closer to optimum value.}
		\includegraphics[width=\textwidth]{gradient-ascent}
	\end{center}
\end{figure}

\begin{align*}
	\frac{d\vec{v}}{dt} \propto& G^T(\vec{u} -G \vec{v}) + \nabla g\\
	\tau \frac{d\vec{v}}{dt} =& G^T\big(\underbrace{\vec{u} -\overbrace{G \vec{v}}^\text{Prediction}}_\text{Error}\big) + \underbrace{\nabla g}_\text{Sparseness constraint}\numberthis\label{eq:maximizeF}
\end{align*}

We can implement this equation as specifying the firing rate dynamics of a recurrent network.

\begin{figure}[H]
	\begin{center}
		\caption[Recurrent network implementation of sparse coding\eqref{eq:maximizeF}]{Recurrent network implementation of sparse coding\eqref{eq:maximizeF}. Network makes a prediction, then calculates an error, which it uses to adjust weights.}
		\includegraphics[width=\textwidth]{recurrent-sparse}
	\end{center}
\end{figure}

We can learn $G$ by applying gradient ascent a second time.

\begin{align*}
	\frac{dG}{dt} \propto& \frac{dF}{dG}\\
	=& \big(\vec{u}-G\vec{v}\big) \vec{v}^T\\
	\tau_G \frac{dG}{dt} =& \big(\vec{u}-G\vec{v}\big) \vec{v}^T \text{. Hebbian, similar to Oja's rule!}
\end{align*}

If $\tau_G>\tau$, $\vec{v}$ will converge faster than $G$, so we can recognize the object $\vec{v}$, then update weights.

If network is similar to Oja's rule, why doesn't it just compute eigenvectors? Because of thr sparseness constraint.
\begin{figure}[H]
	\begin{center}
		\caption[Learning G for Natural Images]{Learning G for Natural Images. Each square is a column $g_i$ of G (obtained by collapsing rows of the square into a vector).	The $g_i$  look like local edge or bar features similar to receptive fields in primary visual cortex (V1)\cite{olshausen1997sparse}.}
		\includegraphics[width=\textwidth]{learning-G-natural}
	\end{center}
\end{figure}

We can interpret this as the brain optimizing its \glspl{gls:rf} to code for natural images in an efficient manner (interpretive model).
\begin{figure}[H]
	\begin{center}
		\caption[Sparse Coding: a special case of Predictive Coding]{Sparse Coding Network is a special case of Predictive Coding Networks. The recurrent weights allow for time dependent input, and the sensory error gain allows for attention\cite{rao1999optimal,rao1999predictive}.}
		\includegraphics[width=\textwidth]{sparse-coding-predictive-coding}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption[Predictive Coding Model of
		the Visual Cortex]{Predictive Coding Model of
			the Visual Cortex\cite{rao1999predictive,gilbert2013top}.  }
		\includegraphics[width=\textwidth]{predictive-coding-visual-cortex}
	\end{center}
\end{figure}

\section{Learning from Supervision \& Rewards}\label{sec:week8}

\subsection{Neurons as Classifiers and Supervised Learning}

\begin{figure}[H]
	\begin{center}
		\caption{The perceptron: a simple classifier, inspired by a neuron}
		\includegraphics[width=\textwidth]{perceptron}
	\end{center}
\end{figure}

The output is given by:
\begin{align*}
	v =& \Theta\big(\sum_i w_i u_i - \mu\big)\text{, where}\\
	\Theta(x) \triangleq& \begin{cases}
		+1, \text{if $x>0$}\\
		-1, \text{if $x\le 0$}
	\end{cases}
\end{align*}
What does a perceptron do? Let is set the expression to zero:
\begin{align*}
	\sum_i w_i u_i - \mu=&0
\end{align*}
This specifies a hyperplane.
\begin{itemize}
	\item All inputs on one side of hyperplane have output = +1 (“class 1”);
	\item all inputs on other side have output = -1 (“class 2”)
\end{itemize}
The perceptron can perform linear classification. How do we learn weights and threshold? We adjust $\{w_i\}$ and $\mu$ according to output error $v^d-v$.
\begin{align*}
	\Delta w_i =& \epsilon(v^d-v)u_i\\
	\Delta \mu =& -\epsilon(v^d-v)
\end{align*}
Unfortunately, A perceptron can only handle linearly separable data; it cannot calculate the XOR function.

\begin{figure}[H]
	\begin{center}
		\caption[A multi-layer perceptron]{A multi-layer perceptron can handle data that isn't separable: her is one that can calculate XOR.}
		\includegraphics[width=\textwidth]{mlp-xor}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption[Multi-layer perceptron with continuous output]{We can achieve continuous output by using a sigmoid}
		\includegraphics[width=\textwidth]{continuous-sigmoid}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption[Learning the weights of an MLP]{Learning the weights of an MLP, given desired output $d$ (Supervised learning)}
		\includegraphics[width=\textwidth]{learn-mlp}
	\end{center}
\end{figure}
We want to minimize the error function
\begin{align*}
	E(W,w) =& \frac{1}{2}\sum_{i} (d_i-v_i)^2 \text{. Using gradient descent}\\
	\delta W_{ij} =& - \epsilon \frac{\partial E}{\partial W_{ij}}\\
	=&\epsilon (d_i-v_i)g^{\prime} (\sum_{j^{\prime}} W_{ij^{\prime}} x_{j^{\prime}}) x_j \text{, which is known as the ``Delta rule''}
\end{align*}

$\delta w_{ij}$ may be computed from the well-known back-propagation rule.

\subsection{Reinforcement Learning: Predicting Rewards}

In contrast with supervised learning we have learning with rewards and punishments.
\begin{figure}[H]
	\begin{center}
		\caption[Reinforcement Learning: rat in barn]{Reinforcement Learning: rat in barn. The agent, Mr. Rat, wants to maximize reward.}
		\includegraphics[width=\textwidth]{rat-barn}
	\end{center}
\end{figure}

\begin{itemize}
	\item Classical Pavlovian conditioning, but...
	\item How do we predict rewards dekivered some time after stimulus is delivered?
	\begin{itemize}
		\item Given: many trials, each of lenght $T$ steps
		\item At time $0\le t\le T$ we have stimulus $u(t)$ and reward $r(t)$, which may be zero.
		\item We want a neuron whose output $v(t)$ predicts the expected tptal future reward starting from time t.  
	\end{itemize}
\end{itemize}

\begin{align*}
	v(t) \approxeq& \Big<\sum_{\tau=0}^{T=t}r(t+\tau)\Big>_{Trials}\text{. Predict based on previous stimuli}\\
	v(t) =& \sum_{\tau=0}^{t}w(\tau)u(t-\tau)\text{. Minimize error}\\
	Error=&\big[\sum_{\tau=0}^{T-t}r_{t+\tau}-v(t)\big]^2
\end{align*}

Problem: we don't have future rewards available! So we rewrite to get rid of future terms.

\begin{align*}
	\big[\sum_{\tau=0}^{T-t}r_{t+\tau}-v(t)\big]^2=&\big[r(t) + \sum_{\tau=0}^{T-t-1}r_{t+1+\tau}-v(t)\big]^2\\
	\approx&\big[r(t)+v(t+1)-v(t)\big]^2\text{, minimize using gradient descent}\\
	\Delta w(\tau)=& \epsilon \big[\overbrace{\underbrace{r(t) + v(t+1)}_\text{Expected future reward}-\underbrace{v(t)}_\text{Prediction}}^\delta \big]u(t-\tau) \numberthis \label{eq:td:learn}
\end{align*}
This is known as Temporal Difference (TD) Learning.

\begin{figure}[H]
	\begin{center}
		\caption[Temporal Difference (TD) Learning]{Temporal Difference (TD) Learning.}
		\includegraphics[width=\textwidth]{td-learning}
	\end{center}
\end{figure}

\begin{figure}[H]
	\caption{Dopaminargenic cells in Ventral Tegmental Area.}
	\begin{subfigure}[t]{0.45\textwidth}
		\caption{Notice change when reward has been learned}
		\includegraphics[width=\textwidth]{reward-primate}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\caption{Effect of failing to deliver reward}
		\includegraphics[width=\textwidth]{no-reward}
	\end{subfigure}
\end{figure}

\subsection{Reinforcement Learning: Time for Action!}

\subsubsection{The Problem: Learn a state-to-action mapping or policy}

\begin{figure}[H]
	\begin{center}
		\caption{The Problem: Learn a state-to-action mapping or policy}		\includegraphics[width=\textwidth]{reward-action-problem}
	\end{center}
\end{figure}

Learn a state-to-action mapping or policy:
\begin{align*}
	\pi(u) =& a \text{ which maximizes the expected total future reward}\\
	\Big<&\sum_{\tau=0}^{T-t}r(t+\tau)\Big>_{trials}
\end{align*}

\begin{figure}[H]
	\begin{center}
		\caption[Example: rat in a barn]{Example: rat in a barn. States = locations A, B, or C. Actions= L (go left) or R (go right)}
		\includegraphics[width=\textwidth]{rat-barn-choices-rewards}
	\end{center}
\end{figure}

For a random policy:
\begin{align*}
	v(B) =&\frac{1}{2}\times 0 + \frac{1}{2}\times 5 =& 2.5\\
	v(C) =&\frac{1}{2}\times 2 + \frac{1}{2}\times 0 =& 1\\
	v(A) =&\frac{1}{2}\times v(B) + \frac{1}{2}\times v(C) =& 1.75
\end{align*}

How does the rat learn values since it goes through states sequentially? We can learn the values using Temporal Difference Learning. We represent the value of each state by a weight, $w(u)$.
\begin{align*}
	w(u) \leftarrow w(u) + \epsilon \big(r(u)+v(u^{\prime})-v(u)\big)\text{, where $u^{\prime}$ is the new state}\\
	(u,a)\rightarrow u^{\prime}	
\end{align*}

\begin{figure}[H]
	\begin{center}
		\caption[Temporal Difference Learning values for random policy]{Temporal Difference Learning values for random policy. For $\epsilon=0.5$ we get fast learning, but a lot of jumps. Once we know the values,  can pick the action that leads to the higher valued state! Values act as surrogate immediate 	rewards $\implies$ Locally optimal choice leads to globally optimal 		policy for Markov environments (Related to Dynamic 	Programming)}
		\includegraphics[width=\textwidth]{td-learning-values}
	\end{center}
\end{figure}

\subsubsection{Putting it all together: Actor-Critic Learning}

Let us see whether we can come up with an algorithm for learning optimal policies in Markov environments. We will have two separate components, the Actor, which selects the action and maintains the policy, and the Critic, which maintains value for each state.
We iterate:
\begin{itemize}
	\item Critic Learning (``Policy Evaluation''). The value is given as in \eqref{eq:td:learn}
	\begin{align*}
		v(u) =& w(u)\\
		w(u) \leftarrow w(u) + \epsilon \big(r(u)+v(u^{\prime})-v(u)\big)
	\end{align*}
	\item Actor Learning (``Policy Improvement''). The Actor selects action $a$ probabilistically, to address the explore/exploit dilemma
	\begin{align*}
		P(a;u) =& \frac{\exp \big(\beta Q_a(u)\big)}{\sum_{a^{\prime}}\exp \big(\beta Q_{a^{\prime}}(u)\big)}\text{. Then update Q for all actions}\\
		Q_{a^{\prime}}(u)\leftarrow & Q_{a^{\prime}}(u) + \epsilon\big[r(u)+v(u^{\prime})-v(u)\big]\big[\delta_{a,a^{\prime}}-P(a^{\prime};u)\big]
	\end{align*}
\end{itemize}




\begin{figure}[H]
	\begin{center}
		\caption[Actor-Critic in Barn Example]{Actor-Critic in Barn Example. After several trials algorithm converges to going left at location A. Notice that C is slow to converge, as algorithm is mostly visiting B.}
		\includegraphics[width=\textwidth]{actor-critic-barn}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption[Possible Implementation of the Actor-Critic
		Model in the Basal Ganglia]{Possible Implementation of the Actor-Critic
			Model in the Basal Ganglia}
		\includegraphics[width=\textwidth]{actor-critic-mapping-brain}
	\end{center}
\end{figure}


\appendix

\printglossaries

% bibliography goes here

\bibliographystyle{unsrt}
\addcontentsline{toc}{section}{Bibliography}
\bibliography{cns}

\end{document}
