\documentclass[]{article}

\usepackage{chronology}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tocloft}
\usepackage{cancel}
\usepackage{thmtools}
\usepackage[toc,nonumberlist,acronym]{glossaries}
\usepackage{glossaries-extra}
\usepackage{relsize}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}[thm]{Lemma}
\graphicspath{{figs/}}
\widowpenalty10000
\clubpenalty10000
\setcounter{tocdepth}{2}

\makeglossaries
%opening
\title{Computational Neuroscience Notes}
\author{Simon Crase}

\begin{document}
\newacronym{gls:ML}{ML\glsadd{gls:mlm}}{\Gls{gls:mlm}}

\newacronym{gls:MAP}{MAP\glsadd{gls:mpm}}{\Gls{gls:mpm}}

\newglossaryentry{gls:mlm}{
	name={maximum likelihood},
	description={Choose estimator $s^*$ to maximize $p[r\vert s]$
		}}
\newglossaryentry{gls:mpm}{
	name={maximum a posteriori},
	description={Choose estimator $s^*$ to maximize $p[s \vert r]$
}}

\newglossaryentry{gls:rf}{
	name={Receptive Field},
	description={Specific properties of a sensory stimulus that generate a strong response from the cell}}
	
\maketitle

\begin{abstract}
My notes from Computational Neuroscience course
\end{abstract}

\tableofcontents
\listoffigures
\section{Introduction and Basic Neurobiology}\label{sec:week1}

\subsection{Course Introduction}
\begin{figure}[H]
	\caption{Our Universes}
	\begin{subfigure}[t]{0.45\textwidth}
		\caption{According to Physicists}
		\includegraphics[width=\textwidth]{universe1}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\caption{According to us}
		\includegraphics[width=\textwidth]{universe2}
	\end{subfigure}
\end{figure}
\subsubsection{Understanding the Brain using Computational Models}
\begin{itemize}
	\item Descriptive Models of the Brain
	\begin{itemize}
		\item How do neurons respond to external stimuli and how do we
		describe this quantitatively with a neural encoding model?
		\item How can we extract information from neurons (decoding)?
	\end{itemize}
    \item Mechanistic Models of Brain Cells and Networks
	\begin{itemize}
		\item     How can we simulate the behavior of a single neuron on a
	    computer?
	    \item How do we simulate a network of neurons?
	\end{itemize}
	\item  Interpretive (or Normative) Models of the Brain
	\begin{itemize}
		\item 	Why do brain circuits operate the way they do?
		\item What are the computational principles underlying their
		operation?
	\end{itemize}
\end{itemize}

\subsubsection{Course Goals: What you can expect to learn}
At the end of the course, you will be able to:
\begin{itemize}
	\item  Quantitatively describe what a biological neuron or
network of neurons is doing given experimental data
 \item Simulate on a computer the behavior of neurons and
networks
 \item Formulate computational principles underlying the
operation of neurons and networks in the brain
\end{itemize}

\subsection{Computational Neuroscience - Descriptive Models}


\begin{figure}[H]
	\caption{Cortical Receptive Fields}
	\includegraphics[width=\textwidth]{CorticalReceptive Fields}
\end{figure}

\subsection{Computational Neuroscience Mechanistic and Interpretive Models}

\subsection{The Electrical Personality of Neurons}

\subsection{Making Connections - Synapses}

\subsection{Time to Network - Brain Areas and their Function}

\begin{figure}[H]
	\caption{Brain Regions}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{Hind Brain. \emph{Medulla Oblongata} controls breathing, muscle tone and blood pressure.
			\emph{Pons} connected to the cerebellum \& involved in sleep and arousal.
			\emph{Cerebellum} Coordination and timing of voluntary movements, sense of equilibrium, language, attention}
		\includegraphics[width=\textwidth]{hindbrain}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{\emph{Midbrain} Eye movements, visual and auditory reflexes.
			\emph{Reticular Formation} 	Modulates muscle reflexes, breathing \& pain perception. Also regulates sleep, 			wakefulness \& 	arousal.}
		\includegraphics[width=\textwidth]{midbrain}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{\emph{Thalamus} Relay station for all 	sensory info (except 	smell) to the cortex, regulates sleep/wakefulness.
		\emph{Hypothalamus} Regulates basic needs: 	Fighting, Fleeing, 	Feeding, and Mating.}
		\includegraphics[width=\textwidth]{thalamus}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\caption{ Consists of: Cerebral cortex, basal ganglia, 	hippocampus, and amygdala
		Involved in perception 	and motor control, 	cognitive functions,	emotion, memory, and learning}
		\includegraphics[width=\textwidth]{Cerebrum}
	\end{subfigure}
\end{figure}
\section{What do Neurons Encode?}\label{sec:week2}
\subsection{What is the Neural Code}
During this course, we will talk about:
\begin{itemize}
	\item techniques for recording from the brain--Figure \ref{fig:recording};
	\item  tools for discovering how the brain represents information;
	\item models that express our understanding of this representation;
	\item  some methods for inferring what the brain is doing based on its activity--Section \ref{sec:week3};
	\item  using information theory to quantify neural representations--Section \ref{sec:week4};
	\item the biophysical basis of how the brain processes inputs and performs complex computations--Section \ref{sec:week5}.
\end{itemize}

\begin{figure}[H]
	\caption[Recording from the Brain]{Recording from the Brain. (\subref{fig:rb1}) \& (\subref{fig:rb2}) fMRI: resolution $1\;mm^3$; response is averaged over many neurons, and is slow. (\subref{fig:rb3}) EEG: response is averaged over many neurons; EEG is faster than fMRI, but noisy. (\subref{fig:rb4}) \& (\subref{fig:rb5}) Electrode Arrays: good if we have access to tissue directly} \label{fig:recording} (\subref{fig:rb6} Calcium Imaging. Cells have markers that change the colour of fluorescence as calcium levels change in response to neural activity.)
	\begin{subfigure}[b]{0.3\textwidth}
		\caption{}\label{fig:rb1}
		\includegraphics[width=0.9\textwidth]{fMRI}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\caption{}\label{fig:rb2}
		\includegraphics[width=0.9\textwidth]{fMRI2}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\caption{}\label{fig:rb3}
		\includegraphics[width=0.9\textwidth]{EEG}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\caption{}\label{fig:rb4}
		\includegraphics[width=0.9\textwidth]{electrode-arrays}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\caption{}\label{fig:rb5}
		\includegraphics[width=0.9\textwidth]{electrode-arrays2}
	\end{subfigure}
	\begin{subfigure}[b]{\textwidth}
		\caption{}\label{fig:rb6}
		\includegraphics[width=0.9\textwidth]{calcium-imaging}
	\end{subfigure}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption[Looking inside a single cell]{Looking inside a single cell. The experimenter clamps a patch electron onto the cell membrane}
		\includegraphics[width=0.9\textwidth]{looking-inside}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption{What is the neural code?}
		\begin{subfigure}[t]{0.45\textwidth}
			\caption{Human eye, without output to optic nerve}
			\includegraphics[width=\textwidth]{human-eye}
		\end{subfigure}
		\begin{subfigure}[to]{0.45\textwidth}
			\caption{Experiment. Play movie while section of retina is connected to electrode array--Figures \ref{fig:rb4} \& \ref{fig:rb5}}
			\includegraphics[width=\textwidth]{human-eye-experiment}
		\end{subfigure}
		\begin{subfigure}[to]{0.45\textwidth}
			\caption{Repeat experiment: red dots represent firings (action potentials). Notice that many times the firings are in almost the same place.}
			\includegraphics[width=\textwidth]{movie-firings}
		\end{subfigure}
		\begin{subfigure}[to]{0.45\textwidth}
			\caption{Look at 20 retinal ganglion cells. Each cell is reasonably consistent in time. Each cell is responsible for encoding some set of features, and different neurons encode different features. R \& P have some features in common.}
			\includegraphics[width=\textwidth]{movie-firings-20}
		\end{subfigure}
	\end{center}
\end{figure}

\begin{itemize}
	\item Encoding: how does a stimulus cause a pattern of responses?
	\item\begin{itemize}
		\item  Building quasi mechanistic models
	\end{itemize}
	\item Decoding: what do these responses tell us about the stimulus?
	\item \begin{itemize}
		\item how can we reconstruct what the brain is doing?
	\end{itemize}
\end{itemize}

\begin{align*}
	P(response\vert stimulus)&\text{, encoding} \numberthis \label{eq:p_r_s}\\
	P(stimulus\vert response)&\text{, decoding}  \numberthis \label{eq:p_s_r}
\end{align*}

\begin{itemize}
	\item What is the stimulus--$s$?
	\item What is the response--$r$?
	\item what is the relation between them?
\end{itemize}
\begin{figure}[H]
	\begin{center}
		\caption[Gaussian tuning curve of a cortical (V1) neuron]{Gaussian tuning curve of a cortical (V1) neuron. These neurons respond to oriented bars.}
		\includegraphics[width=0.9\textwidth]{tuning-curves}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption{Cosine tuning curve of a motor cortical neuron}
		\includegraphics[width=0.9\textwidth]{tuning-curves1}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption{Feature Map in Primary Visual Cortex}
		\includegraphics[width=0.9\textwidth]{feature-map}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption{Higher order features in temporal lobe}
		\includegraphics[width=0.9\textwidth]{higher-order-features}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption[Building up more complex features]{Building up more complex features. Notice feed-forward and feed-back.}
		\includegraphics[width=0.9\textwidth]{build}
	\end{center}
\end{figure}

\subsection{Neural Encoding - Simple Models}e firing rate, 

We  consider the case where a response is a single spike. We want to determine $P(R\vert S) \rightarrow r(t)$, where $r(t)$ is the firing rate, given a stimulus $s$.

\begin{figure}[H]
	\begin{center}
		\caption{Simplest possible model: $r(t) = \phi s(t-\tau)$)}
		\includegraphics[width=0.9\textwidth]{simplest-model}
	\end{center}
\end{figure}

What about a linear filter?

\begin{align*}
	r(t) =& \sum_{i=1}^{n} s_{t-i} f_i \text{, or spatial filtering} \numberthis \label{eq:linear:filter}\\
	r(x,y) =& \sum_{x^\prime=-n,y^\prime=-n}^{n,n}s_{x-x^\prime,y-y^\prime} f_{x^\prime,y^\prime} \text{, as in Figure \ref{fig:rf1}}
\end{align*}

\begin{figure}[H]
	\begin{center}
		\caption[Receptive Field]{Receptive Field: (\subref{fig:rf1}) is a cartoon view, (\subref{fig:rf2}) shows the filter as a difference between two Gaussians; (\subref{fig:rf4}) shows the effect of applying the filter to (\subref{fig:rf3}.)}
		\begin{subfigure}[t]{0.45\textwidth}
			\caption{Cartoon}\label{fig:rf1}
			\includegraphics[width=0.9\textwidth]{receptive-field1}
		\end{subfigure}
		\begin{subfigure}[t]{0.45\textwidth}
			\caption{Detailed view}\label{fig:rf2}
			\includegraphics[width=0.9\textwidth]{receptive-field2}
		\end{subfigure}
		\begin{subfigure}[t]{0.45\textwidth}
			\caption{Taj Mahal}\label{fig:rf3}
			\includegraphics[width=0.9\textwidth]{receptive-field3}
		\end{subfigure}
		\begin{subfigure}[t]{0.45\textwidth}
			\caption{Filtered Taj Mahal, showing edges only}\label{fig:rf4}
			\includegraphics[width=0.9\textwidth]{receptive-field4}
		\end{subfigure}
	\end{center}
\end{figure}

\begin{figure}[H]
	\caption[Spatiotemporal filtering]{Spatiotemporal filtering. $r_{x,y}(t)=\int dx^\prime dy^\prime d\tau f(x^\prime,y^\prime,\tau) s((x-x^\prime,y-y^\prime,t-\tau)$}
	\includegraphics[width=0.9\textwidth]{spatio--temporal-filtering}
\end{figure}

Linear filters cannot be the full picture, however, as they have a few limitations:
\begin{itemize}
	\item they can give rise to a negative firing rate!
	\item they can increase indefinitely!
\end{itemize} 
We need to impose a non-linearity, so \eqref{eq:linear:filter} becomes:
\begin{align*}
	r(t) =& g\big(\sum_{i=1}^{n} s_{t-i} f_i\big) 
\end{align*}

\begin{figure}[H]
	\caption{Filter showing effect of saturating non-linearity}
	\includegraphics[width=0.9\textwidth]{saturated}
\end{figure}

\subsection{Neural Encoding - Feature Selection}
\subsection{Neural Encoding - Variability}
\section{Extracting Information from Neurons}\label{sec:week3}

\subsection{Neural Decoding and Signal Detection Theory}

\begin{figure}[H]
		\caption[Value of threshold that maximizes probability of correct call]{This value of threshold maximizes probability of calling correctly.
			$p[+]p[r\ge z\vert +] + p[-] (1-p[r\ge z\vert -])$}
		\includegraphics[width=\textwidth]{signal-detection1}
\end{figure}
\subsection{Population Coding and Bayesian Estimating}

\begin{align*}
	\underbrace{p[s\vert r]}_\text{A posteriori distribution} =& \frac{\overbrace{p[r\vert s]}^\text{Likelihood function} \cdot \overbrace{p[s]}^\text{Prior distribution}}{\underbrace{p[r]}_\text{Marginal distribution}} \text{, where}\\
	p[r] =& \int ds \; p[r\vert s] p[s]
\end{align*}

\begin{itemize}
	\item \gls{gls:ML} \gls{gls:mlm}
	\item \gls{gls:MAP} \gls{gls:mpm}
\end{itemize}
 An example. Assume:
 
\begin{enumerate}
	\item a population of neurons that encode some stimulus $s$
	\item response is Gaussian (e.g. V1)
	\item each fires independently
	\item Poisson Firing\label{item:poisson}
\end{enumerate}

\begin{figure}[H]
	\caption{Gaussian Tuning Curves}
	\includegraphics[width=\textwidth]{decode-stimulus}
\end{figure}

\begin{align*}
	f_a(s) =& r_{max} \exp \Big(-\frac{1}{2}\big[\frac{s-s_a}{\sigma_a}\big]^2\Big)\text{, assume good coverage} \numberthis \label{eq:gauss}\\
	\sum_{1}^{N} f_a(s) =& const \numberthis \label{eq:good:coverage}
\end{align*}

So firing rate doesn't depend on stimulus

From assumption \ref{item:poisson}, spikes are produced randomly and independently in each time bin with probability
\begin{align*}
	P_T[k] =& \frac{(rT)^k \exp (-rT)}{k!}\\
	P_T[r_a\vert s] =& \frac{(f_a(s)T)^{r_aT} \exp (-f_a(s)T)}{r_aT!}\\
	P[\vec{r}\vert s] =& \prod_{a=1}^{N}P_r[a\vert s]\\
	=& \prod_{a=1}^{N} \frac{(f_a(s)T)^{r_aT} \exp (-f_a(s)T)}{r_aT!}
\end{align*}

We want the \gls{gls:ML} for $s$.
\begin{align*}
	\ln P[\vec{r}\vert s] =& \sum_{a=1}^{N} \big[r_aT \ln (f_a(s)T) -f_a(s)T - \ln (r_aT!) \big] \text{, so we need}\\
	\nabla_a \ln P[\vec{r}\vert s] =& 0	
\end{align*}
 where  $\nabla_a$ denotes the operator $\frac{\partial}{\partial_{s_a}}$. Now
\begin{align*}
	\nabla_a \ln P[\vec{r}\vert s] = & \partial_a \sum_{a=1}^{N}  r_aT \ln (f_a(s)T) -\underbrace{\partial_a \sum_{a=1}^{N} f_a(s)T}_{=0 \text { from ]\eqref{eq:good:coverage}}} - \underbrace{\partial_a \sum_{a=1}^{N} \ln (r_aT!)}_{=0} 
\end{align*}
Hence the \gls{gls:mlm} is given by:
\begin{align*}
	T \partial_a \sum_{a=1}^{N} r_a  \ln (f_a(s)T) =& 0\\
	 \sum_{a=1}^{N} r_a  \partial_a \ln (f_a(s)T) =& 0\\
	 \sum_{a=1}^{N} r_a  \frac{\partial_a  (f_a(s)\cancel{T})}{(f_a(s)\cancel{T})} =& 0\\
	 \sum_{a=1}^{N} r_a  \frac{\partial_a  f_a(s^*)}{f_a(s^*)} =& 0 \numberthis \label{eq:ml:example}
\end{align*}

\begin{align*}
	\partial_a  f_a(s) =& \partial_a r_{max} \exp \Big(-\frac{1}{2}\big[\frac{s-s_a}{\sigma_a}\big]^2\Big)\\
	=&r_{max}\big[-\frac{1}{\cancel{2}} \frac{\cancel{2}(s-s_a)}{\sigma_a^2}\big] \exp \Big(-\frac{1}{2}\big[\frac{s-s_a}{\sigma_a}\big]^2\Big)\\
	=& \frac{\cancel{2}(s-s_a)}{\sigma_a^2} f_a(s) \text{, so \eqref{eq:ml:example} becomes}
\end{align*}
\begin{align*}
	\sum_{a=1}^{N} r_a  \frac{(s^*-s_a)}{\sigma_a^2} =& 0\\
	s^*  =& \frac{\sum_{a=1}^{N}   \frac{s_a r_a}{\sigma_a^2}}{\sum_{a=1}^{N} \frac{ r_a }{\sigma_a^2}}	\\
	=& \frac{\sum_{a=1}^{N}   p_a s_a r_a}{\sum_{a=1}^{N} p_a r_a} \text{, where the precision $p_a=\sigma_a^{-2}$}\\
	=& \frac{\sum_{a=1}^{N}   s_a r_a}{\sum_{a=1}^{N} r_a} \text{, if all the $s_a$ are equal}
\end{align*}

For the \gls{gls:MAP} estimate, Bayes rule gives:
\begin{align*}
	\ln p[s|r] =& \ln p[r|s] + \ln p[s] - \ln p[r]\\
	=& T \sum_{a=1}^{N}r_a \ln f_a(s) + \ln p[s] + C \text{, where $C$ does not depend on $s_a$}
\end{align*}

So the \gls{gls:MAP} estimator satisfies:
\begin{align*}
	\sum_{a=1}^{N} r_a \frac{f^\prime(a^*)}{f(a^*)} + \frac{p^\prime(s)}{p(s)}=&0\\
	s^*  =& \frac{\sum_{a=1}^{N}   \frac{s_a r_a}{\sigma_a^2}+\frac{s_{prior}}{\sigma_{prior}^2}}{\sum_{a=1}^{N} \frac{ r_a }{\sigma_a^2}+\frac{1}{\sigma_{prior}^2}}
\end{align*}
\subsection{Reading Minds - Stimulus Reconstruction}
We want an estimator, $S_{Bayes}$ that gives the "best" estimate of $s$ given $r$.

\section{Entropy \& Spike Trains}\label{sec:week4}

\subsection{Information \& Entropy}

Entropy measures surprise.
\begin{align*}
	H =& - \sum_{i} p_i \log_2 p_i
\end{align*}

\begin{figure}[H]
	\begin{center}
		\caption{How about the stimulus?}
		\includegraphics[width=0.8\textwidth]{how}\label{fig:how:about:the:stimulus}
	\end{center}
\end{figure}

In Figure \ref{fig:how:about:the:stimulus}, suppose the probability of error is a constant.
\begin{align*}
	P[r_-\vert +] =& q\\
	P[r_+\vert +] =& 1-q\\
	P[r_+\vert -] =& q\\
	P[r_-\vert -] =& 1-q
\end{align*}
\begin{align*}
	H[R] =& -P[r_+] \log P[r_+] - P[r_-] \log P[r_-] \text{, total entropy}\\
	H[R\vert +] =& -q \log q - (1-q) \log (1-q) \text{, noise entropy}
\end{align*}


Mutual Information: total entropy - average noise entropy.

\begin{align*}
	I(R,S) =& -\sum_r p[r] \log p[r] - \sum_s p(s)\big[- \sum_r p[r \vert s] \log p[r \vert s]\big]\\
	=& H[R] - \sum_s p(s) H[R\vert s]\\
\end{align*}
\begin{align*}
	I[S,R] \triangleq D_{KL}&\big[P(R,S),P(R)P(S)\big]\\
	=&H[R]-\sum_s P(s) H[R\vert s]
\end{align*}

\subsection{Calculating Information in Spike Trains}

What information is carried by patterns of spikes?

Mutual  information  is  the  difference  between  
the  total  response  entropy  and   the  mean  noise  entropy.
\begin{align*}
	I(S;R) =& H[R] - \sum_s P[s] H[R\vert s]
\end{align*}
\begin{figure}[H]
	\begin{center}
			\caption[Calculating Information in Spike Patterns]{Calculating Information in Spike Patterns. Divide into words of length $T$, e.g. $w_1=[1,0,1,0,1,1,0]$. Information is difference between total variability driven by stimuli and that dues to noise, averaged over stimuli.\cite{strong1998entropy}}
		\includegraphics[width=\textwidth]{calculatingInformationInSpikePatterns}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption[Entropy for chunked words]{Entropy for chunked words. The most common pattern is all zeroes, then a single bit, etc. nformation  :   
			difference  between  the  total  
			variability   driven  by  stimuli  
			and  that  due  to   noise,  averaged  
			over  stimuli. \cite{strong1998entropy,reinagel2000temporal}}
		\includegraphics[width=\textwidth]{info-spike-trains}
	\end{center}
\end{figure}
\subsection{Coding Principles}
\begin{itemize}
	\item What are the challenges posed by natural stimuli?
	\item What do information theoretic concepts suggest that neural systems should do?
	\item What principle seems to be at work shaping the neuarl code?
\end{itemize}

Photo - have to change F-stop repeatedly in order to capture details of scene inside and outside; the eye does this effortlessly. Natural stimuli:
\begin{enumerate}
	\item Have a wide dynamic range: variations over many orders of magnitude;
	\item dynamic scaling.
\end{enumerate}

\begin{figure}[H]
	\begin{center}
		\caption[Power Law]{Power Law: there are similar structures at very different length scales.}
		\begin{subfigure}[b]{0.45\textwidth}
			\caption{Structures}
			\includegraphics[width=0.9\textwidth]{power-law-graph}
		\end{subfigure}
		\begin{subfigure}[b]{0.45\textwidth}
			\caption{Power Spectrum: there is no characteristic scale for image.}
			\includegraphics[width=0.9\textwidth]{power-law-shapes}
		\end{subfigure}
	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\caption[Efficient Coding]{Efficient Coding: In order to have maximum entropy output, a good encoder should match its outputs to the distribution of its inputs. Try to use all symbols equally often. (cumulative probability)}
		\includegraphics[width=0.8\textwidth]{efficient-coding}
	\end{center}
\end{figure}

Contrast varies widely in time. Should neural system optimize locally or over evolutionary time?

\begin{figure}[H]
	\begin{center}
		\caption{What have we left out}
		\includegraphics[width=0.8\textwidth]{4-big-picture}
	\end{center}
\end{figure}

Make sure we have covered
\begin{enumerate}
	\item Efficient coding
	\item Sparse Coding
\end{enumerate}

\begin{align*}
	I(\vec{x}) =& \sum_{i}a_i \phi_i(\vec{x}) + \epsilon(\vec{x})\\
	E =& \mathlarger \sum_{\vec{x}}\big[I(\vec{x})-\sum_{i}a_i \phi_i(\vec{x})\big]^2 + \lambda \sum_{i}C(a_i)
\end{align*}
\section{Computing in Carbon}\label{sec:week5}

\section{Computing with Networks}\label{sec:week6}

\section{Networks that Learn}\label{sec:week7}

\section{Learning from Supervision \& Rewards}\label{sec:week8}

\appendix

\printglossaries

% bibliography goes here

\bibliographystyle{unsrt}
\addcontentsline{toc}{section}{Bibliography}
\bibliography{cns}

\end{document}
